{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9ca44f2d-0e69-4fae-8af3-47fcf2cc2524",
   "metadata": {},
   "outputs": [],
   "source": [
    "##in this version i will say screw pipelines and just do it without for now ... \n",
    "#maybe I will see what this infinity issue is if I can inspect the resuly of each step manually"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b1560cfb-6589-4bfe-9b8f-8103a1fa7925",
   "metadata": {},
   "outputs": [],
   "source": [
    "##Round 1 cleaning complete. v1 decisions: keep all nan as new feature/value; no skew or outlier action has been taken\n",
    "##Round 1 exploration complete. (a) Distribution of each feature explored (b) feature-target and feature-feature relationships explored \n",
    "\n",
    "##Round 1 modelling Plan: make one model for each notebook, try to do systematic tuning ... take best few models (?) or is it too soon?\n",
    "##then go back and do feature engineering and feature selection based on these base-line models\n",
    "##Build pipe-line: dummies, scaling, tuning ...\n",
    "\n",
    "##next steps:\n",
    "##\n",
    "##Flask API Deployment\n",
    "##build small webpage interaction\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1359efb1-103d-4360-a79d-3fceb18e9e4e",
   "metadata": {},
   "source": [
    "### Tuning of KNN \n",
    "\n",
    "Main parameters: k = num_neighbors, and metric"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8a8ea7be-9e6e-4828-a3b3-c987068772ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "## following https://machinelearningmastery.com/hyperparameters-for-classification-machine-learning-algorithms/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6d3930bc-de41-4da9-b866-9925090d5c99",
   "metadata": {},
   "outputs": [],
   "source": [
    "##brief review of KNN vs K-means ... \n",
    "##K-means (unspervised) has no labels, just feature space ... you create k-labels (choice of k is tricky and important)\n",
    "##then run thru k-means clustering algorithm to assign k-labels (clusters) to the data \n",
    "##b\n",
    "##KNN (supervised) is simpler because data already comes with p-labels (the role of k is different here) \n",
    "##in the training data on the test data you simply look at the k-nearet neighors (of training set) and take majority vote \n",
    "##on what label to assign to new point. The choice of k is important and the metric can change the outcomes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d36736eb-dfca-4e86-9fb5-5ac94b4893b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "from matplotlib import pyplot as plt\n",
    "\n",
    "from sklearn.preprocessing import FunctionTransformer\n",
    "from sklearn.compose import ColumnTransformer\n",
    "\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.preprocessing import StandardScaler, RobustScaler, QuantileTransformer, OrdinalEncoder\n",
    "from sklearn.feature_selection import SelectKBest, f_regression\n",
    "from sklearn.decomposition import PCA \n",
    "from sklearn.linear_model import Ridge\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, roc_auc_score, confusion_matrix\n",
    "from sklearn.metrics import mean_absolute_error, mean_squared_error, r2_score, f1_score\n",
    "import seaborn as sns\n",
    "\n",
    "import pickle\n",
    "\n",
    "#sns.set()\n",
    "sns.set(rc={'figure.figsize':(8,5)})\n",
    "\n",
    "#! very useful \n",
    "\n",
    "pd.set_option('display.max_columns', 500)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "480bb674-3d82-4edd-ab40-d9a0549b871c",
   "metadata": {},
   "outputs": [],
   "source": [
    "##baseline models\n",
    "from sklearn.model_selection import train_test_split # For train/test splits\n",
    "from sklearn.neighbors import KNeighborsClassifier # The k-nearest neighbor classifier\n",
    "from sklearn.feature_selection import VarianceThreshold # Feature selector\n",
    "from sklearn.pipeline import Pipeline # For setting up pipeline\n",
    "# Various pre-processing steps\n",
    "from sklearn.preprocessing import Normalizer, StandardScaler, MinMaxScaler, PowerTransformer, MaxAbsScaler, LabelEncoder\n",
    "\n",
    "from sklearn.model_selection import RepeatedStratifiedKFold\n",
    "from sklearn.model_selection import RandomizedSearchCV\n",
    "\n",
    "from sklearn.model_selection import GridSearchCV # For optimization\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "504d2b7e-24b6-4be4-b6a8-035ba0005dd4",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from sklearn.model_selection import RepeatedStratifiedKFold\n",
    "from sklearn.model_selection import RandomizedSearchCV\n",
    "\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.linear_model import LogisticRegression \n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "#rfc = RandomForestClassifier(max_depth=4, random_state=0)\n",
    "#gnb = GaussianNB()\n",
    "#lgc = LogisticRegression()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "98fec286-67d0-43e2-9388-f752a6f0cffc",
   "metadata": {},
   "outputs": [],
   "source": [
    "## cleaning tool\n",
    "\n",
    "\n",
    "def perc_null(X):\n",
    "    \n",
    "    total = X.isnull().sum().sort_values(ascending=False)\n",
    "    data_types = X.dtypes\n",
    "    percent = (X.isnull().sum()/X.isnull().count()).sort_values(ascending=False)\n",
    "\n",
    "    missing_data = pd.concat([total, data_types, percent], axis=1, keys=['Total','Type' ,'Percent'])\n",
    "    return missing_data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d19da237-ff89-4db6-a057-e268ecef85bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "#general functions ... testing done in notebook Proj1_Loans_Test_Imputers_Scalers\n",
    "\n",
    "def most_freq_imputer(df, col_list): #data =df, list_col = [col1, col2, ...] columns of df\n",
    "    for col in col_list:\n",
    "        most_freq_val = df[col].value_counts().index[0]  #0 grabs most frequent\n",
    "        df.loc[df[col].isnull(), col] = most_freq_val\n",
    "\n",
    "\n",
    "def mean_imputer(df, col_list): #data =df, list_col = [col1, col2, ...] columns of df\n",
    "    for col in col_list:\n",
    "        mean = round(np.mean(df[col]),2)\n",
    "        df.loc[df[col].isnull(), col] = mean\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "97129bdf-ab66-4382-8dd4-59e9fd504c6d",
   "metadata": {},
   "outputs": [],
   "source": [
    "#max_min scaler\n",
    "\n",
    "def max_min_helper(x, min, max, new_min =0, new_max=1):\n",
    "    y = (x-min)/(max-min)   #scales to min = 0, max = 1\n",
    "    scaled = y*(new_max-new_min) + new_min \n",
    "    return scaled\n",
    "#scale by max \n",
    "\n",
    "def max_min_scaler(df, col_list, new_min =0, new_max=1):\n",
    "    for col in col_list:\n",
    "\n",
    "        min = np.min(df[col])\n",
    "        max = np.max(df[col])\n",
    "        \n",
    "        df[col] = df[col].map(lambda x: max_min_helper(x, min, max, new_min, new_max))\n",
    "                   \n",
    "    \n",
    "##why is this one not working? because you had max, min arguments switched in your function call"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a7d674fe-a16c-42b4-89fd-db785fc279b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "#mean scaler\n",
    "\n",
    "\n",
    "def mean_scaler(df, col_list):\n",
    "    for col in col_list:\n",
    "        mean = np.mean(df[col])\n",
    "        sigma = np.std(df[col]) #std deviation\n",
    "        df[col] = (df[col] - mean)/sigma\n",
    "              "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "84ed78ec-e3a5-42c6-978d-f79aa86412ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "path = \"/Users/joejohns/Downloads/data_loans.csv\"\n",
    "\n",
    "data = pd.read_csv(path) \n",
    "print(data.shape)\n",
    "data.head(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "63641633-d69d-4139-b4c9-6c46878e7554",
   "metadata": {},
   "source": [
    "## data processing from round 1\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "753f03c8-e92b-4f38-8474-f52e6e5d7f7c",
   "metadata": {},
   "outputs": [],
   "source": [
    "perc_null(data)\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bae93a29-a088-45ec-b8a9-25f032363043",
   "metadata": {},
   "outputs": [],
   "source": [
    "nan_feats = list(perc_null(data)[ perc_null(data)['Total'] >0].index) \n",
    "#nan_feats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f80c7b0a-593c-4b8e-a460-f09ab9d24362",
   "metadata": {},
   "outputs": [],
   "source": [
    "data.rename(columns = {'ApplicantIncome':'Applicant_Income', 'CoapplicantIncome': 'Coapplicant_Income', 'LoanAmount':'Loan_Amount'}, inplace = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "559e805f-16fc-4026-b313-491a1701a735",
   "metadata": {},
   "outputs": [],
   "source": [
    "target = ['Loan_Status']\n",
    "ID_feats = ['Loan_ID',] \n",
    "\n",
    "#feature types (this must be done manually ... although data types would give a pretty good approximation)\n",
    "\n",
    "numer_feats = ['Applicant_Income', 'Coapplicant_Income', 'Loan_Amount', 'Loan_Amount_Term',]\n",
    "\n",
    "ord_cat_feats = [ 'Dependents', 'Education', ]\n",
    "nom_cat_feats = ['Gender', 'Married', 'Self_Employed', 'Credit_History', 'Property_Area',]\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9f2a5ba2-7672-4c5f-bb96-f9327869ad2e",
   "metadata": {},
   "outputs": [],
   "source": [
    "data.index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5590a563-55e5-4a45-9f37-132d4b268da3",
   "metadata": {},
   "outputs": [],
   "source": [
    "#data.loc[:, ord_cat_feats].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "589e71bf-0f44-436e-ab03-afa63592f71e",
   "metadata": {},
   "outputs": [],
   "source": [
    "ord_labels = [ [ '0','1','2','3+' ], ['Not Graduate', 'Graduate']]  #consistent with order of categories in ord_cat_feats (and in df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "631c4fb0-4309-4269-bbf7-f7750ad773bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "#fix target; make it numeric; useful for regression modelling\n",
    "\n",
    "data.loc[data[\"Loan_Status\"] == 'Y', \"Loan_Status\"] = 1\n",
    "data.loc[data[\"Loan_Status\"] == 'N', \"Loan_Status\"] = 0\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4b73e4da-77a9-4d88-a5a8-0b702ac04ae5",
   "metadata": {},
   "outputs": [],
   "source": [
    "#fix this nom cat ... float --> string\n",
    "\n",
    "data.loc[data['Credit_History'] ==0.0, 'Credit_History'] = 'No'\n",
    "data.loc[data['Credit_History'] == 1.0, 'Credit_History'] = 'Yes'  #I use capitalized \"Yes\", \"No\" to be conistent with their other entries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7496c6e1-90b7-4411-b98f-8e778a683a9d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "8809f871-07f7-460a-83f1-9742f8fcfba7",
   "metadata": {},
   "source": [
    "impute_most_freq =[]\n",
    "\n",
    "for feat in numer_feats:\n",
    "    perc_val = data[feat].value_counts()/data.shape[0]\n",
    "    if perc_val.iloc[0] >=0.7:\n",
    "        impute_most_freq+=[feat]\n",
    "    \n",
    "\n",
    "A = set(ord_cat_feats+numer_feats) \n",
    "B = set(nan_feats)\n",
    "C = set(impute_most_freq)\n",
    "impute_mean_feats = list(A-C)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8ed74415-1b8f-4096-9b53-28c8373910a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "\n",
    "\n",
    "impute_freq_feats = ['Loan_Amount_Term', 'Dependents']  #how can we identify this automatically? It's a cts var where over X% of values = one value \n",
    "impute_mean_feats = ['Loan_Amount'] \n",
    " \n",
    "scale_feats = ord_cat_feats+numer_feats\n",
    "\n",
    "#can make a list comprehension (rather than loop) that extracts these ... \n",
    "unk_cat_feats = [ 'Loan_Amount', 'Loan_Amount_Term', 'Dependents', ] #make new category for these 'unk' = yes/no; ##impute by mean\n",
    "\n",
    "#this does not work with OHE\n",
    "#nom_cat_nan_dummies = ['Credit_History', 'Self_Employed', 'Gender', 'Married',]  ##make new dumies cat_val for nan\n",
    "\n",
    "#update this one for OHE\n",
    "dummies_feats = ['Property_Area','missing_data']+['Credit_History', 'Self_Employed', 'Gender', 'Married',]  #regular dummies\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9232a0f2-33c0-413f-b417-de03fa343984",
   "metadata": {},
   "outputs": [],
   "source": [
    "impute_mean_feats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bd966d5d-20a3-4b7d-8f67-b040a9a709e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "#change all nan to 'unk' in the categoricals ...\n",
    "for feat in set(nan_feats)&set(dummies_feats):\n",
    "    data.loc[data[feat].isnull(),  feat ] = 'unk'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "544e69e0-8169-4864-a5e9-c9e89af51f93",
   "metadata": {},
   "outputs": [],
   "source": [
    "set(nan_feats)&set(dummies_feats)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a22352aa-32ab-406d-a59a-d30746a47949",
   "metadata": {},
   "outputs": [],
   "source": [
    "perc_null(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fa8ad176-a4e1-4a66-a014-d8af99a01ed5",
   "metadata": {},
   "outputs": [],
   "source": [
    "##carry out imputation and scaling steps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "804b83ea-e508-4cb1-84d9-d0632c16a5d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "##evebtually perhaps we can make this into a class that can be used in a pipe line ... \n",
    "\n",
    "#step 1. create new categorical (records if any null values in any of specified cats)\n",
    "\n",
    "data['missing_data'] = 'No'  \n",
    "for feature in unk_cat_feats:\n",
    "    data.loc[(data[feature].isnull()) , 'missing_data'] = 'Yes'\n",
    "\n",
    "    \n",
    "nom_cat_feats+=['missing_data']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d0783b92-e910-421d-b90f-0fac5eda6dfa",
   "metadata": {},
   "outputs": [],
   "source": [
    "##now we execute all imputing and scaling ... bruh\n",
    "\n",
    "\n",
    "#steps:\n",
    "\n",
    "#1. DONE create new cat recording if NaN in specified numeric or nominal features \n",
    "#(do this before imputing of course)\n",
    "\n",
    "#numeric and nominal: \n",
    "#2. impute most_freq (Loan_Term)\n",
    "#3. impute mean (everyone else)\n",
    "#4. scale by max, min (or by mean; try both)\n",
    "\n",
    "#nominal cats:\n",
    "#5. get_dummies, nan yes\n",
    "#6. get_dummies, nan no\n",
    "\n",
    "#perc_null(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ef4733ae-b50d-4629-95cb-d58fb49fe11a",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_feats = list(set(perc_null(data).index) - set(target+ID_feats))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "869994bb-9563-4777-a292-f9b66f5afbd1",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_feats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "df067ddd-b9f2-40ac-9223-d094b5e4d15b",
   "metadata": {},
   "outputs": [],
   "source": [
    "##now let's make a pipe line ... first with the normal skl functions ... then with my functions "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "640b75c0-5778-4d3f-ad9c-f3531f4db502",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.preprocessing import OneHotEncoder, OrdinalEncoder\n",
    "from sklearn.preprocessing import MinMaxScaler, StandardScaler\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "45970e84-d397-41c8-8715-66b2f67a0366",
   "metadata": {},
   "outputs": [],
   "source": [
    "impute_most_freq = SimpleImputer(strategy = 'most_frequent')\n",
    "impute_mean = SimpleImputer(strategy = 'mean')\n",
    "\n",
    "ordinals = OrdinalEncoder(categories = ord_labels)\n",
    "\n",
    "scaler_mean =StandardScaler()\n",
    "scaler_max = MinMaxScaler()\n",
    "\n",
    "dummies = OneHotEncoder() #(sparse = False)  #allowing it to create sparse matrix for large matrix can save a lot of memory\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "849aec51-a689-400d-9d3a-f7500d138aae",
   "metadata": {},
   "outputs": [],
   "source": [
    "col_most_freq = ColumnTransformer([\n",
    "('impute_most_freq', impute_most_freq, impute_freq_feats), \n",
    "], remainder='passthrough')\n",
    "\n",
    "col_impute_mean = ColumnTransformer([\n",
    "('impute_mean', impute_mean, impute_mean_feats),     \n",
    "], remainder='passthrough')\n",
    "\n",
    "col_ord_enc = ColumnTransformer([\n",
    "('ordinals', ordinals, ord_cat_feats),\n",
    "],  remainder='passthrough')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fad7165e-f49c-4904-a243-7974d3783bc2",
   "metadata": {},
   "outputs": [],
   "source": [
    "preprocessor = Pipeline([\n",
    "   ('col_most_freq', col_most_freq), ('col_impute_mean', col_impute_mean), ('col_ord_enc', col_ord_enc )\n",
    "])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4aefb97e-d9fb-483a-a4ad-23e5cf1bcc6f",
   "metadata": {},
   "source": [
    "\n",
    "preprocessor = ColumnTransformer([\n",
    "('impute_most_freq', impute_most_freq, impute_freq_feats), \n",
    "('impute_mean', impute_mean, impute_mean_feats),     \n",
    "('ordinals', ordinals, ord_cat_feats),\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dd53f8fa-4f22-4c82-b93c-1d9ef361a973",
   "metadata": {},
   "outputs": [],
   "source": [
    "preprocessor.fit_transform(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e318ef3b-a4c4-43cb-ae2b-93cd43bf48a7",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4ee0a83f-effa-4078-82ad-dc238ab8abed",
   "metadata": {},
   "outputs": [],
   "source": [
    "#impute most freq (test)\n",
    "\n",
    "impute_most_freq.fit_transform(data[impute_freq_feats])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "883aa9eb-d85c-4d48-bd79-38fc0817426c",
   "metadata": {},
   "outputs": [],
   "source": [
    "#impute mean (test)\n",
    "\n",
    "impute_mean.fit_transform(data[impute_mean_feats])[0:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "17c1a84e-3512-4ac5-9f5d-a1037d252584",
   "metadata": {},
   "outputs": [],
   "source": [
    "data.dropna()[0:3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5a1c7afc-dd40-4f6c-957f-be4c9bc14c49",
   "metadata": {},
   "outputs": [],
   "source": [
    "data['Dependents'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "60bde92b-7940-4749-bf26-bedb2e029934",
   "metadata": {},
   "outputs": [],
   "source": [
    "ordinals.fit_transform(data[ord_cat_feats].dropna())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aba8a85c-264f-45b4-b9cc-d07d4dc68360",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9c59e64e-dbbf-4675-b893-0fd2a5e7930d",
   "metadata": {},
   "outputs": [],
   "source": [
    "#scaler_max.fit_transform(data[scale_feats])  #this cannot work until imputing done ...  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fe2bf951-6812-440d-9857-8f668bc77d9d",
   "metadata": {},
   "outputs": [],
   "source": [
    "dummies.fit_transform(data[dummies_feats])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1e4941e4-28c0-40ec-a48d-3643ea655a5a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "011ea416-41cf-4163-bd1f-d395b5818410",
   "metadata": {},
   "outputs": [],
   "source": [
    "#let me now follow https://machinelearningmastery.com/modeling-pipeline-optimization-with-scikit-learn/\n",
    "#which does cv = 3 instead of cv = stratififedkfold( ... )\n",
    "\n",
    "\n",
    "y = data_mean.loc[:, target]\n",
    "X= data_mean.loc[:, model_feats]\n",
    "y = y.astype(int)\n",
    "# Split the data into test and train\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X,  \n",
    "    y, \n",
    "    test_size=1/3,\n",
    "    random_state=0)\n",
    " \n",
    "print(X_train.shape)\n",
    "print(X_test.shape)\n",
    "\n",
    "#np.array(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "54d3b09a-ed5b-4321-9826-9de479282289",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5582b265-decb-4559-9526-0c4cbe68637f",
   "metadata": {},
   "outputs": [],
   "source": [
    "knn = KNeighborsClassifier()\n",
    "\n",
    "knn.fit(X_train, np.ravel(y_train))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7ea3f87c-5c84-4380-b452-0a8a19906f66",
   "metadata": {},
   "outputs": [],
   "source": [
    "knn.score(X_test, np.ravel(y_test))  #77% ? shitty! ha ha was 80% in my original version"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e9d58e4c-d0a1-44ab-bb90-596a75e627d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "##Sat Sept 18, 1:30 pm ... make pipe line? or do tuning and feature selection? "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f57a9cea-98b6-48b5-a0a9-f7e971cf4b36",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.linear_model import LogisticRegression \n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "rfc = RandomForestClassifier(max_depth=4, random_state=0)\n",
    "gnb = GaussianNB()\n",
    "lgc = LogisticRegression()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "96a3945a-6fff-4544-95c8-544240126f37",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "rfc.fit(X_train, np.ravel(y_train))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5bac71bd-fe79-4347-8ce2-a90f42907937",
   "metadata": {},
   "outputs": [],
   "source": [
    "rfc.score(X_test, np.ravel(y_test))  ##aha! I used rfc before and it IS improved to 82% ... cool. That makes me feel better ... (all that effort should not make it worse)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8f1c8a26-ab46-4119-95d1-8b607d9a7ee3",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "lgc.fit(X_train, np.ravel(y_train))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "28d30bba-3df5-4ef6-835d-c8bbea2f38a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "lgc.score(X_test, np.ravel(y_test))  ##aha! I used rfc before and it IS improved to 82% ... cool. That makes me feel better ... (all that effort should not make it worse)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7335f49f-5b0b-485f-8aaf-50d2d8fa91a6",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
