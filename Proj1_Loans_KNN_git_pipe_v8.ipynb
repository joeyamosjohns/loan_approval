{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7370a4f0-f6d1-4b93-ac56-f44563653a97",
   "metadata": {},
   "outputs": [],
   "source": [
    "##! ah you need to make *disjoint* subsets of features and run thru *all* steps on each subset ... then feature union them "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9ca44f2d-0e69-4fae-8af3-47fcf2cc2524",
   "metadata": {},
   "outputs": [],
   "source": [
    "##in this version i will say screw pipelines and just do it without for now ... \n",
    "#maybe I will see what this infinity issue is if I can inspect the resuly of each step manually\n",
    "\n",
    "##I will now follow Julie Michelman and Zac Stewart's approach to pipelines:\n",
    "'''Zac does feature union a lot\n",
    "Julie stays in panda land by creating custom transformers\n",
    "neither uses column transformers'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b1560cfb-6589-4bfe-9b8f-8103a1fa7925",
   "metadata": {},
   "outputs": [],
   "source": [
    "##Round 1 cleaning complete. v1 decisions: keep all nan as new feature/value; no skew or outlier action has been taken\n",
    "##Round 1 exploration complete. (a) Distribution of each feature explored (b) feature-target and feature-feature relationships explored \n",
    "\n",
    "##Round 1 modelling Plan: make one model for each notebook, try to do systematic tuning ... take best few models (?) or is it too soon?\n",
    "##then go back and do feature engineering and feature selection based on these base-line models\n",
    "##Build pipe-line: dummies, scaling, tuning ...\n",
    "\n",
    "##next steps:\n",
    "##\n",
    "##Flask API Deployment\n",
    "##build small webpage interaction\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1359efb1-103d-4360-a79d-3fceb18e9e4e",
   "metadata": {},
   "source": [
    "### Tuning of KNN \n",
    "\n",
    "Main parameters: k = num_neighbors, and metric"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8a8ea7be-9e6e-4828-a3b3-c987068772ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "## following https://machinelearningmastery.com/hyperparameters-for-classification-machine-learning-algorithms/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6d3930bc-de41-4da9-b866-9925090d5c99",
   "metadata": {},
   "outputs": [],
   "source": [
    "##brief review of KNN vs K-means ... \n",
    "##K-means (unspervised) has no labels, just feature space ... you create k-labels (choice of k is tricky and important)\n",
    "##then run thru k-means clustering algorithm to assign k-labels (clusters) to the data \n",
    "##b\n",
    "##KNN (supervised) is simpler because data already comes with p-labels (the role of k is different here) \n",
    "##in the training data on the test data you simply look at the k-nearet neighors (of training set) and take majority vote \n",
    "##on what label to assign to new point. The choice of k is important and the metric can change the outcomes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "d36736eb-dfca-4e86-9fb5-5ac94b4893b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "from matplotlib import pyplot as plt\n",
    "\n",
    "from sklearn.preprocessing import FunctionTransformer\n",
    "from sklearn.compose import ColumnTransformer\n",
    "\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.preprocessing import StandardScaler, RobustScaler, QuantileTransformer, OrdinalEncoder\n",
    "from sklearn.feature_selection import SelectKBest, f_regression\n",
    "from sklearn.decomposition import PCA \n",
    "from sklearn.linear_model import Ridge\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, roc_auc_score, confusion_matrix\n",
    "from sklearn.metrics import mean_absolute_error, mean_squared_error, r2_score, f1_score\n",
    "import seaborn as sns\n",
    "\n",
    "import pickle\n",
    "\n",
    "#sns.set()\n",
    "sns.set(rc={'figure.figsize':(8,5)})\n",
    "\n",
    "#! very useful \n",
    "\n",
    "pd.set_option('display.max_columns', 500)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "480bb674-3d82-4edd-ab40-d9a0549b871c",
   "metadata": {},
   "outputs": [],
   "source": [
    "##baseline models\n",
    "from sklearn.model_selection import train_test_split # For train/test splits\n",
    "from sklearn.neighbors import KNeighborsClassifier # The k-nearest neighbor classifier\n",
    "from sklearn.feature_selection import VarianceThreshold # Feature selector\n",
    "from sklearn.pipeline import Pipeline # For setting up pipeline\n",
    "# Various pre-processing steps\n",
    "from sklearn.preprocessing import Normalizer, StandardScaler, MinMaxScaler, PowerTransformer, MaxAbsScaler, LabelEncoder\n",
    "\n",
    "from sklearn.model_selection import RepeatedStratifiedKFold\n",
    "from sklearn.model_selection import RandomizedSearchCV\n",
    "\n",
    "from sklearn.model_selection import GridSearchCV # For optimization\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "504d2b7e-24b6-4be4-b6a8-035ba0005dd4",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from sklearn.model_selection import RepeatedStratifiedKFold\n",
    "from sklearn.model_selection import RandomizedSearchCV\n",
    "\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.linear_model import LogisticRegression \n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "#rfc = RandomForestClassifier(max_depth=4, random_state=0)\n",
    "#gnb = GaussianNB()\n",
    "#lgc = LogisticRegression()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "98fec286-67d0-43e2-9388-f752a6f0cffc",
   "metadata": {},
   "outputs": [],
   "source": [
    "## cleaning tool\n",
    "\n",
    "\n",
    "def perc_null(X):\n",
    "    \n",
    "    total = X.isnull().sum().sort_values(ascending=False)\n",
    "    data_types = X.dtypes\n",
    "    percent = (X.isnull().sum()/X.isnull().count()).sort_values(ascending=False)\n",
    "\n",
    "    missing_data = pd.concat([total, data_types, percent], axis=1, keys=['Total','Type' ,'Percent'])\n",
    "    return missing_data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "84ed78ec-e3a5-42c6-978d-f79aa86412ad",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(614, 13)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Loan_ID</th>\n",
       "      <th>Gender</th>\n",
       "      <th>Married</th>\n",
       "      <th>Dependents</th>\n",
       "      <th>Education</th>\n",
       "      <th>Self_Employed</th>\n",
       "      <th>ApplicantIncome</th>\n",
       "      <th>CoapplicantIncome</th>\n",
       "      <th>LoanAmount</th>\n",
       "      <th>Loan_Amount_Term</th>\n",
       "      <th>Credit_History</th>\n",
       "      <th>Property_Area</th>\n",
       "      <th>Loan_Status</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>LP001002</td>\n",
       "      <td>Male</td>\n",
       "      <td>No</td>\n",
       "      <td>0</td>\n",
       "      <td>Graduate</td>\n",
       "      <td>No</td>\n",
       "      <td>5849</td>\n",
       "      <td>0.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>360.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>Urban</td>\n",
       "      <td>Y</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>LP001003</td>\n",
       "      <td>Male</td>\n",
       "      <td>Yes</td>\n",
       "      <td>1</td>\n",
       "      <td>Graduate</td>\n",
       "      <td>No</td>\n",
       "      <td>4583</td>\n",
       "      <td>1508.0</td>\n",
       "      <td>128.0</td>\n",
       "      <td>360.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>Rural</td>\n",
       "      <td>N</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>LP001005</td>\n",
       "      <td>Male</td>\n",
       "      <td>Yes</td>\n",
       "      <td>0</td>\n",
       "      <td>Graduate</td>\n",
       "      <td>Yes</td>\n",
       "      <td>3000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>66.0</td>\n",
       "      <td>360.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>Urban</td>\n",
       "      <td>Y</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    Loan_ID Gender Married Dependents Education Self_Employed  \\\n",
       "0  LP001002   Male      No          0  Graduate            No   \n",
       "1  LP001003   Male     Yes          1  Graduate            No   \n",
       "2  LP001005   Male     Yes          0  Graduate           Yes   \n",
       "\n",
       "   ApplicantIncome  CoapplicantIncome  LoanAmount  Loan_Amount_Term  \\\n",
       "0             5849                0.0         NaN             360.0   \n",
       "1             4583             1508.0       128.0             360.0   \n",
       "2             3000                0.0        66.0             360.0   \n",
       "\n",
       "   Credit_History Property_Area Loan_Status  \n",
       "0             1.0         Urban           Y  \n",
       "1             1.0         Rural           N  \n",
       "2             1.0         Urban           Y  "
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "path = \"/Users/joejohns/Downloads/data_loans.csv\"\n",
    "\n",
    "data = pd.read_csv(path) \n",
    "print(data.shape)\n",
    "data.head(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "63641633-d69d-4139-b4c9-6c46878e7554",
   "metadata": {},
   "source": [
    "## data processing from round 1\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "753f03c8-e92b-4f38-8474-f52e6e5d7f7c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Total</th>\n",
       "      <th>Type</th>\n",
       "      <th>Percent</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>Credit_History</th>\n",
       "      <td>50</td>\n",
       "      <td>float64</td>\n",
       "      <td>0.081433</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Self_Employed</th>\n",
       "      <td>32</td>\n",
       "      <td>object</td>\n",
       "      <td>0.052117</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>LoanAmount</th>\n",
       "      <td>22</td>\n",
       "      <td>float64</td>\n",
       "      <td>0.035831</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Dependents</th>\n",
       "      <td>15</td>\n",
       "      <td>object</td>\n",
       "      <td>0.024430</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Loan_Amount_Term</th>\n",
       "      <td>14</td>\n",
       "      <td>float64</td>\n",
       "      <td>0.022801</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Gender</th>\n",
       "      <td>13</td>\n",
       "      <td>object</td>\n",
       "      <td>0.021173</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Married</th>\n",
       "      <td>3</td>\n",
       "      <td>object</td>\n",
       "      <td>0.004886</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Loan_ID</th>\n",
       "      <td>0</td>\n",
       "      <td>object</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Education</th>\n",
       "      <td>0</td>\n",
       "      <td>object</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>ApplicantIncome</th>\n",
       "      <td>0</td>\n",
       "      <td>int64</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>CoapplicantIncome</th>\n",
       "      <td>0</td>\n",
       "      <td>float64</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Property_Area</th>\n",
       "      <td>0</td>\n",
       "      <td>object</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Loan_Status</th>\n",
       "      <td>0</td>\n",
       "      <td>object</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                   Total     Type   Percent\n",
       "Credit_History        50  float64  0.081433\n",
       "Self_Employed         32   object  0.052117\n",
       "LoanAmount            22  float64  0.035831\n",
       "Dependents            15   object  0.024430\n",
       "Loan_Amount_Term      14  float64  0.022801\n",
       "Gender                13   object  0.021173\n",
       "Married                3   object  0.004886\n",
       "Loan_ID                0   object  0.000000\n",
       "Education              0   object  0.000000\n",
       "ApplicantIncome        0    int64  0.000000\n",
       "CoapplicantIncome      0  float64  0.000000\n",
       "Property_Area          0   object  0.000000\n",
       "Loan_Status            0   object  0.000000"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "perc_null(data)\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "559e805f-16fc-4026-b313-491a1701a735",
   "metadata": {},
   "outputs": [],
   "source": [
    "#fix feature names\n",
    "\n",
    "data.rename(columns = {'ApplicantIncome':'Applicant_Income', 'CoapplicantIncome': 'Coapplicant_Income', 'LoanAmount':'Loan_Amount'}, inplace = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "631c4fb0-4309-4269-bbf7-f7750ad773bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "#fix target; make it numeric; useful for regression modelling\n",
    "\n",
    "data.loc[data[\"Loan_Status\"] == 'Y', \"Loan_Status\"] = 1\n",
    "data.loc[data[\"Loan_Status\"] == 'N', \"Loan_Status\"] = 0\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "4b73e4da-77a9-4d88-a5a8-0b702ac04ae5",
   "metadata": {},
   "outputs": [],
   "source": [
    "#fix this nom cat ... float --> string\n",
    "\n",
    "data.loc[data['Credit_History'] ==0.0, 'Credit_History'] = 'No'\n",
    "data.loc[data['Credit_History'] == 1.0, 'Credit_History'] = 'Yes'  #I use capitalized \"Yes\", \"No\" to be conistent with their other entries"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8809f871-07f7-460a-83f1-9742f8fcfba7",
   "metadata": {},
   "source": [
    "impute_most_freq =[]\n",
    "\n",
    "for feat in numer_feats:\n",
    "    perc_val = data[feat].value_counts()/data.shape[0]\n",
    "    if perc_val.iloc[0] >=0.7:\n",
    "        impute_most_freq+=[feat]\n",
    "    \n",
    "\n",
    "A = set(ord_cat_feats+numer_feats) \n",
    "B = set(nan_feats)\n",
    "C = set(impute_most_freq)\n",
    "impute_mean_feats = list(A-C)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "f357a59d-6645-45fb-9694-e7ddf4acd0de",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['Credit_History', 'Self_Employed', 'Loan_Amount', 'Dependents',\n",
       "       'Loan_Amount_Term', 'Gender', 'Married'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "nan_feats = perc_null(data)[ perc_null(data)['Total'] >0].index\n",
    "\n",
    "nan_feats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ad2a9c07-88a1-4ee8-8925-bbe431b9cad4",
   "metadata": {},
   "outputs": [],
   "source": [
    "sorted(list(data['Loan_Amount_Term'].value_counts(dropna = False).index))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d3ee4e21-c51a-496c-863d-3101d79c4eac",
   "metadata": {},
   "outputs": [],
   "source": [
    "data['Loan_Amount_Term'].value_counts(dropna = False).sort_index()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e2a9083d-1dbe-4356-9c3f-72cf48aa04ed",
   "metadata": {},
   "source": [
    "#proposed categories:\n",
    "361-500    15\n",
    "240-360    512+4+13\n",
    "121-239     44     \n",
    "60-120     2+4+3\n",
    "0-59       1+2\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "2a1cde34-abed-4c8c-8954-094459eb8b2f",
   "metadata": {},
   "outputs": [],
   "source": [
    "#make new Loan_Term ord_cat\n",
    "\n",
    "data['Loan_Term_bin']=pd.cut(x = data['Loan_Amount_Term'],\n",
    "                        bins = [0,60,120,240,360,500],  #this is [0,60], (60, 120], ...\n",
    "                        labels = [1,2,3,4,5])                    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "6592fb96-9e58-41c8-90c8-af3a8b58bbc1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1        5\n",
       "2        7\n",
       "3       48\n",
       "4      525\n",
       "5       15\n",
       "NaN     14\n",
       "Name: Loan_Term_bin, dtype: int64"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data['Loan_Term_bin'].value_counts(dropna = False).sort_index()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "8a31e905-81f5-4904-a62a-9e315b9ce43e",
   "metadata": {},
   "outputs": [],
   "source": [
    "#let's drop \n",
    "\n",
    "data.drop( labels=['Loan_Amount_Term'], axis =1, inplace = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "5805a117-f8d2-4196-9169-052bccabe6ae",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Credit_History', 'Self_Employed', 'Loan_Amount', 'Dependents', 'Loan_Term_bin', 'Gender', 'Married']\n"
     ]
    }
   ],
   "source": [
    "\n",
    "nan_feats = list(perc_null(data)[ perc_null(data)['Total'] >0].index) \n",
    "#nan_feats\n",
    "print(nan_feats)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "8ed74415-1b8f-4096-9b53-28c8373910a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "nan_feats = ['Credit_History', 'Self_Employed', 'Gender', 'Married' ,    # nom_cat ---> Nan = new cat_val\n",
    "          'Loan_Amount',    #record in unk_cat and impute mean\n",
    "             'Dependents',  'Loan_Term_bin' ]   #record in unk_cat and impute most_freq \n",
    "\n",
    "target = ['Loan_Status']\n",
    "ID_feats = ['Loan_ID',] \n",
    "\n",
    "#feature types (this must be done manually ... although data types would give a pretty good approximation)\n",
    "\n",
    "numer_feats = ['Applicant_Income', 'Coapplicant_Income', 'Loan_Amount', ]\n",
    "nom_cat_feats = ['Gender', 'Married', 'Self_Employed', 'Credit_History', 'Property_Area',]\n",
    "ord_cat_feats = [ 'Dependents', 'Education', 'Loan_Term_bin' ]\n",
    "\n",
    "#steps \n",
    "\n",
    "#nom_cats; record nans, then do dummies\n",
    "\n",
    "impute_nom_cats_unk = [x for x in nom_cat_feats if x in nan_feats] \n",
    "add_dummies = nom_cat_feats\n",
    "\n",
    "\n",
    "#ord_cats: add new dummy var for ord and numer feats: \"feat_unk\" = 0,1 according if NaN\n",
    "#then do impute, ord_encode, scale \n",
    "\n",
    "make_dummies_ord = [x for x in ord_cat_feats if x in nan_feats]\n",
    "impute_freq = [x for x in ord_cat_feats if x in nan_feats]\n",
    "# labels for ordinal encoding (must be done by hand for correct order):\n",
    "\n",
    "ord_labels_dic = {  'Dependents':  { '0':0,'1':1,'2':2,'3+':3 }, 'Education': {'Not Graduate':0, 'Graduate':1}, 'Loan_term_bin': \"Pass\"}\n",
    "scale_ord = ord_cat_feats #I will use X --> X/max{|X|}\n",
    "\n",
    "##numer feats \n",
    "\n",
    "make_dummies_numer =  [x for x in ord_cat_feats if x in nan_feats]\n",
    "impute_mean = [x for x in numer_feats if x in nan_feats]\n",
    "scale_numer = numer_feats #I will use X --> X/max{|X|}\n",
    "\n",
    "#ord_encode = [ 'Dependents', 'Education'] #Loan_term_bin is already ord encoded \n",
    "#ord_labels = [ [ '0','1','2','3+' ], ['Not Graduate', 'Graduate']]  #consistent with order of categories in ord_cat_feats (and in df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f4d89d1b-cdbc-4989-905e-1e1144875155",
   "metadata": {},
   "outputs": [],
   "source": [
    "#test ...looks good \n",
    "\n",
    "data2  = data.head(10).copy()\n",
    "data2.replace(to_replace = ord_labels_dic['Education'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "78851ae1-1557-4982-b2bc-6605a7fa33cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "#check these worked properly ... looks good \n",
    "\n",
    "print(impute_nom_cats_unk, \n",
    "make_dummies_nan, \n",
    "impute_freq, \n",
    "impute_mean )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bd966d5d-20a3-4b7d-8f67-b040a9a709e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "#change all nan to 'unk' in the categoricals ...\n",
    "#for feat in set(nan_feats)&set(dummies_feats):\n",
    "#    data.loc[data[feat].isnull(),  feat ] = 'unk'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9eadb76f-a042-4007-9da8-24a654080336",
   "metadata": {},
   "source": [
    "##eventually perhaps we can make this into a class that can be used in a pipe line ... \n",
    "\n",
    "#step 1. create new categorical (records if any null values in any of specified cats)\n",
    "\n",
    "def update_missing(x, feature):\n",
    "    if x == 'No':\n",
    "        return feature\n",
    "    else: \n",
    "        return x+'&'+feature\n",
    "\n",
    "data['missing_data'] = 'No'  \n",
    "for feature in unk_cat_feats:\n",
    "    data.loc[(data[feature].isnull()) , 'missing_data'] = data.loc[(data[feature].isnull()) , 'missing_data'].map(lambda x: update_missing(x, feature))\n",
    "\n",
    "    \n",
    "nom_cat_feats+=['missing_data']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "21bbe255-7810-481d-b8fb-440a213c4118",
   "metadata": {},
   "outputs": [],
   "source": [
    "#testing len(data.loc[(data['Education'].isnull()), 'Education'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12c0f3da-67a1-4e54-8ca0-22fa99b02534",
   "metadata": {},
   "outputs": [],
   "source": [
    "#make_dummies_nan_new_feats = ['missing_'+x for x in make_dummies_nan]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dcaa3458-adaf-4cbe-acf1-b98259f39d25",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "5998debd-c558-4596-be93-4aa79b1815ed",
   "metadata": {},
   "source": [
    "for feature in unk_cat_feats:\n",
    "    if len(data.loc[(data[feature].isnull()), :].count()) > 0:\n",
    "        data['missing_'+feature] = 0  #dummy var\n",
    "        data.loc[(data[feature].isnull()) , 'missing_'+feature] = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f4139e37-c7ea-4025-8c32-1ecfb2f69b52",
   "metadata": {},
   "outputs": [],
   "source": [
    "for feature in unk_cat_feats:\n",
    "    print(data['missing_'+feature].value_counts())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6034bfb5-9898-424a-bff3-2b8d5754f686",
   "metadata": {},
   "source": [
    "##now we execute all imputing and scaling ... bruh\n",
    "\n",
    "\n",
    "#steps:\n",
    "\n",
    "#1. DONE create new cat recording if NaN in specified numeric or nominal features \n",
    "#(do this before imputing of course)\n",
    "\n",
    "#numeric and nominal: \n",
    "#2. impute most_freq (Loan_Term)\n",
    "#3. impute mean (everyone else)\n",
    "#4. scale by max, min (or by mean; try both)\n",
    "\n",
    "#nominal cats:\n",
    "#5. get_dummies, nan yes\n",
    "#6. get_dummies, nan no\n",
    "\n",
    "#perc_null(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b9c6dd0e-ee50-45fb-9804-ce315a1becc7",
   "metadata": {},
   "outputs": [],
   "source": [
    "set(data.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ef4733ae-b50d-4629-95cb-d58fb49fe11a",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_feats = list(set(data.columns+unk_cat_dummies) - set(target+ID_feats))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "869994bb-9563-4777-a292-f9b66f5afbd1",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_feats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "df067ddd-b9f2-40ac-9223-d094b5e4d15b",
   "metadata": {},
   "outputs": [],
   "source": [
    "##now let's make a pipe line ... first with the normal skl functions ... then with my functions "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "640b75c0-5778-4d3f-ad9c-f3531f4db502",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.preprocessing import OneHotEncoder, OrdinalEncoder\n",
    "from sklearn.preprocessing import MinMaxScaler, StandardScaler\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4aefb97e-d9fb-483a-a4ad-23e5cf1bcc6f",
   "metadata": {},
   "source": [
    "\n",
    "preprocessor = ColumnTransformer([\n",
    "('impute_most_freq', impute_most_freq, impute_freq_feats), \n",
    "('impute_mean', impute_mean, impute_mean_feats),     \n",
    "('ordinals', ordinals, ord_cat_feats),\n",
    "])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d1b7b0d4-2373-4a59-99d3-7a3b2c3e0c49",
   "metadata": {},
   "source": [
    "Steps I will follow\n",
    "\n",
    "- make new dummy_vars for the non nom_cat missing NaN  #this tracks all missing NaN for us ... is it worth all the dummies? Make numeric/ordinal?\n",
    "- ordinal encode (NaN just leave for now; impute plus recorded in new_cat ... does not make sense to make ordinal Nan ... -1? no)\n",
    "- dummies (Nan ---> new cat val) # must come after make new cat ... \n",
    "- impute (freq = ord plus numer1; mean numer2)  # must come after ordinal encode \n",
    "- scale (numer and ord ... X ---> X/|max X|).   # I think X --> X - X_min is an unneccsary distortion\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9872d445-97ed-49a4-9d35-65694c0d8b78",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from functools import reduce\n",
    "\n",
    "from sklearn.base import TransformerMixin\n",
    "from sklearn.feature_extraction import DictVectorizer\n",
    "from sklearn.preprocessing import FunctionTransformer, StandardScaler, RobustScaler, MultiLabelBinarizer\n",
    "from sklearn.impute import SimpleImputer\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "380f7acf-104b-40e1-bd62-abc480a4763c",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "class ColumnExtractor(TransformerMixin):\n",
    "\n",
    "    def __init__(self, cols):\n",
    "        self.cols = cols\n",
    "\n",
    "    def fit(self, X, y=None):\n",
    "        # stateless transformer\n",
    "        return self\n",
    "\n",
    "    def transform(self, X):\n",
    "        # assumes X is a DataFrame\n",
    "        Xcols = X[self.cols]\n",
    "        return Xcols\n",
    "    \n",
    "class DummyTransformer(TransformerMixin):\n",
    "\n",
    "    def __init__(self):\n",
    "        self.dv = None\n",
    "\n",
    "    def fit(self, X, y=None):\n",
    "        # assumes all columns of X are strings\n",
    "        Xdict = X.to_dict('records')\n",
    "        self.dv = DictVectorizer(sparse=False)\n",
    "        self.dv.fit(Xdict)\n",
    "        return self\n",
    "\n",
    "    def transform(self, X):\n",
    "        # assumes X is a DataFrame\n",
    "        Xdict = X.to_dict('records')\n",
    "        Xt = self.dv.transform(Xdict)\n",
    "        cols = self.dv.get_feature_names()\n",
    "        Xdum = pd.DataFrame(Xt, index=X.index, columns=cols)\n",
    "        # drop column indicating NaNs\n",
    "        nan_cols = [c for c in cols if '=' not in c]\n",
    "        Xdum = Xdum.drop(nan_cols, axis=1)\n",
    "        return Xdum"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f41cfc49-5f27-4d7b-807b-a8f908a59be5",
   "metadata": {},
   "outputs": [],
   "source": [
    "#we assume columns have been restricted to the ordinal columns \n",
    "\n",
    "class DFOrdinalEncoder(TransformerMixin):  #transformerMixin just gives us fit_transform automatically\n",
    "    # FunctionTransformer but for pandas DataFrames\n",
    "    #def __init__(self): ! appears we don't need to do init\n",
    "    \n",
    "    def fit(self, X, y=None):\n",
    "        # stateless transformer\n",
    "        return self\n",
    "\n",
    "    def transform(self, X):\n",
    "        # assumes X is a DataFrame\n",
    "        \n",
    "        return Xdate\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "038e4afe-0cd5-4ac0-8c3a-4370a0266881",
   "metadata": {},
   "outputs": [],
   "source": [
    "class DFImputeNanAsUnk(TransformerMixin):\n",
    "    def __init__(self):  #cols = list of features (strings)\n",
    "    \n",
    "    def fit(self, X, y=None):\n",
    "        # stateless transformer\n",
    "        return self\n",
    "\n",
    "    def transform(self, X):\n",
    "        Xt=X.copy()\n",
    "        for feat in Xt.columns: \n",
    "            Xt.loc[Xt[feat].isnull(),  feat ] = 'unk'\n",
    "        return Xt\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "593683d8-3cc6-4e3d-9e44-51a695ec057a",
   "metadata": {},
   "outputs": [],
   "source": [
    "class DFAddDummiesNaN(TransformerMixin):\n",
    "    \n",
    "    def __init__(self, cols):  #cols = list of features (strings)\n",
    "        self.cols = cols\n",
    "        \n",
    "    def fit(self, X, y = None):\n",
    "        return self \n",
    "    \n",
    "    def transform(self, X):\n",
    "        Xt = X.copy()\n",
    "        for feature in self.cols:\n",
    "            if len(data.loc[(Xt[feature].isnull()), :].count()) > 0:\n",
    "                Xt['missing_'+feature] = 0  #dummy var\n",
    "                Xt.loc[(Xt[feature].isnull()) , 'missing_'+feature] = 1\n",
    "        return Xt\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "27a5c7c1-2e28-4491-8096-69a46edfed53",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "#nom_cats; record nans, then do dummies\n",
    "\n",
    "impute_nom_cats_unk = [x for x in nom_cat_feats if x in nan_feats] \n",
    "add_dummies = nom_cat_feats\n",
    "\n",
    "\n",
    "#ord_cats: add new dummy var for ord and numer feats: \"feat_unk\" = 0,1 according if NaN\n",
    "#then do impute, ord_encode, scale \n",
    "\n",
    "make_dummies_ord = [x for x in ord_cat_feats if x in nan_feats]\n",
    "impute_freq = [x for x in ord_cat_feats if x in nan_feats]\n",
    "# labels for ordinal encoding (must be done by hand for correct order):\n",
    "ord_encode = [ 'Dependents', 'Education'] #Loan_term_bin is already ord encoded \n",
    "ord_labels = [ [ '0','1','2','3+' ], ['Not Graduate', 'Graduate']]  #consistent with order of categories in ord_cat_feats (and in df)\n",
    "scale_ord = ord_cat_feats #I will use X --> X/max{|X|}\n",
    "\n",
    "##numer feats \n",
    "\n",
    "make_dummies_numer =  [x for x in ord_cat_feats if x in nan_feats]\n",
    "impute_mean = [x for x in numer_feats if x in nan_feats]\n",
    "scale_numer = numer_feats #I will use X --> X/max{|X|}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "859fd3ee-d9a1-4f66-bc8d-b72837390fbf",
   "metadata": {},
   "outputs": [],
   "source": [
    "##without pipeline first:\n",
    "\n",
    "\n",
    "def most_freq_imputer(df, col_list): #data =df, list_col = [col1, col2, ...] columns of df\n",
    "    for col in col_list:\n",
    "        most_freq_val = df[col].value_counts().index[0]  #0 grabs most frequent\n",
    "        df.loc[df[col].isnull(), col] = most_freq_val\n",
    "\n",
    "\n",
    "def mean_imputer(df, col_list): #data =df, list_col = [col1, col2, ...] columns of df\n",
    "    for col in col_list:\n",
    "        mean = round(np.mean(df[col]),2)\n",
    "        df.loc[df[col].isnull(), col] = mean\n",
    "\n",
    "\n",
    "\n",
    "def ord_encode(X,labels_dic): #labels_dic = {\"F1\": {F1_val1: 1, F1_val2: 2, ...}, ..., F3: \"Pass\"} F3 ord_var needs no processing\n",
    "    for feature in labels_dic.keys():\n",
    "        if labels_dic[feature] == \"Pass\":\n",
    "            continue\n",
    "        else:\n",
    "            X2 = X.replace(to_replace = labels_dic[feature]).copy()\n",
    "    return X2\n",
    "        \n",
    "def make_nan_unk(X,col_list):\n",
    "    X2 = X.copy()  #this could be slow if X is big\n",
    "    for feature in col_list:\n",
    "        X2.loc[X[feature].isnull(), feature] = \"unk\"\n",
    "    return X2     \n",
    "\n",
    "def make_nan_dummy(X, col_list):\n",
    "    X2 = X.copy()\n",
    "    for feature in col_list:\n",
    "        X2[feature+\"_unk\"] = 0\n",
    "        X2.loc[X[feature].isnull(), :] = 1\n",
    "\n",
    "def max_scaler(X, col_list):  #assumes no NaN; o/w will return NaN\n",
    "    X2 = X.copy()\n",
    "    for feature in col_list: \n",
    "        abs_max = np.max([np.abs(x) for x in X2[feature] ] )\n",
    "        X2[feature] = X2[feature]/abs_max   #rescales so quantities between -1 and 1\n",
    "\n",
    "def make_dummies(X, col_list):\n",
    "    X2 = X.copy()\n",
    "    for feature in col_list:\n",
    "        feat_vals = set(X[feature]).copy()\n",
    "        for feat_val in feat_vals: \n",
    "            #X2[feat_val+'_dummy'] = 0\n",
    "            X2[feat_val+'_dummy'] = (X2[feature] == feat_val).map(lambda x: int(x))\n",
    "    X2.drop(labels = col_list, axis = 1, inplace = True) #drop the dummified nom_cat variables\n",
    "    return X2\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f023abd7-89e6-47be-9a76-7a70046048c6",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b7d440d3-28ae-46ce-986d-b3a5830b0c29",
   "metadata": {},
   "outputs": [],
   "source": [
    "#no pipe line\n",
    "\n",
    "#nom_cats; record nans, then do dummies\n",
    "\n",
    "impute_nom_cats_unk = [x for x in nom_cat_feats if x in nan_feats] \n",
    "add_dummies = nom_cat_feats\n",
    "\n",
    "\n",
    "#ord_cats: add new dummy var for ord and numer feats: \"feat_unk\" = 0,1 according if NaN\n",
    "#then do impute, ord_encode, scale \n",
    "\n",
    "make_dummies_ord = [x for x in ord_cat_feats if x in nan_feats]\n",
    "impute_freq = [x for x in ord_cat_feats if x in nan_feats]\n",
    "# labels for ordinal encoding (must be done by hand for correct order):\n",
    "\n",
    "ord_labels_dic = {  'Dependents':  { '0':0,'1':1,'2':2,'3+':3 }, 'Education': {'Not Graduate':0, 'Graduate':1}, 'Loan_term_bin': \"Pass\"}\n",
    "scale_ord = ord_cat_feats #I will use X --> X/max{|X|}\n",
    "\n",
    "##numer feats \n",
    "\n",
    "make_dummies_numer =  [x for x in ord_cat_feats if x in nan_feats]\n",
    "impute_mean = [x for x in numer_feats if x in nan_feats]\n",
    "scale_numer = numer_feats #I will use X --> X/max{|X|}\n",
    "\n",
    "X = data.copy()\n",
    "\n",
    "#nom_cats:\n",
    "\n",
    "t = [('num', SimpleImputer(strategy='median'), [0, 1]), \n",
    "     ('cat', SimpleImputer(strategy='most_frequent'), [2, 3])]\n",
    "transformer = ColumnTransformer(transformers=t, remainder='passthrough')\n",
    "#transformer = ColumnTransformer(transformers=[('cat', OneHotEncoder(), [2, 3])], remainder='passthrough')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "6ae05444-2193-4a5e-927a-dfe187e6e88b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "nan"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.max([np.abs(x) for x in data[\"Loan_Amount\"] ] )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "0194c1c3-46c7-4369-a5c4-6da1106d707f",
   "metadata": {},
   "outputs": [],
   "source": [
    "#make new Loan_Term ord_cat\n",
    "\n",
    "data['Loan_Term_bin']=pd.cut(x = data['Loan_Amount_Term'],\n",
    "                        bins = [0,60,120,240,360,500],  #this is [0,60], (60, 120], ...\n",
    "                        labels = [1,2,3,4,5])                    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "be2b2917-77a8-4661-a28a-28d49b848962",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1        5\n",
       "2        7\n",
       "3       48\n",
       "4      525\n",
       "5       15\n",
       "NaN     14\n",
       "Name: Loan_Term_bin, dtype: int64"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data['Loan_Term_bin'].value_counts(dropna = False).sort_index()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5e03572a-4855-4183-8a9f-d996a1ae3183",
   "metadata": {},
   "outputs": [],
   "source": [
    "pipeline = Pipeline([\n",
    "    ('feature_processing', \n",
    "     FeatureUnion([\n",
    "         ('nom_cats_processing', \n",
    "              Pipeline([\n",
    "                 ('extract', ColumnExtractor(nom_cat_feats))\n",
    "                 ('impute_nom_cats_unk', impute_nom_cats_unk)  \n",
    "                 ('dummies', DummyTransformer)\n",
    "                 ]))\n",
    "        ('ord_cats_processing', \n",
    "              Pipeline([\n",
    "                 ('extract', ColumnExtractor(ord_cat_feats))\n",
    "                 ('make_unk_cat_vars', DFAddDummiesNaN())\n",
    "                 ('impute_freq', Imputer type = freq)\n",
    "                 ('ord_encode', OrdEncoder)   #need the labels here ... \n",
    "                 ('scale_ord', Scaler(custom by max) ) \n",
    "                 ]))\n",
    "        ('numer_processing', \n",
    "              Pipeline([\n",
    "                 ('extract', ColumnExtractor(numer_cat_feats))\n",
    "                 ('make_unk_cat_vars', DFAddDummiesNaN())\n",
    "                 ('impute_mean', Imputer type = mean)\n",
    "                 ('scale_numer', Scaler(custom by max) ) \n",
    "                 ]))\n",
    "    ])        \n",
    "        \n",
    "])\n",
    "    \n",
    "    \n",
    "    ('make_dummies_nan',  DFAddDummiesNaN(cols = make_dummies_nan),\n",
    "    ('features', FeatureUnion([\n",
    "        ('continuous', Pipeline([\n",
    "            ('extract', ColumnExtractor(CONTINUOUS_FIELDS)),\n",
    "            ('scale', Normalizer())\n",
    "        ])),\n",
    "        ('factors', Pipeline([\n",
    "            ('extract', ColumnExtractor(FACTOR_FIELDS)),\n",
    "            ('one_hot', OneHotEncoder(n_values=5)),\n",
    "            ('to_dense', DenseTransformer())\n",
    "        ])),\n",
    "        ('weekday', Pipeline([\n",
    "            ('extract', DayOfWeekTransformer()),\n",
    "            ('one_hot', OneHotEncoder()),\n",
    "            ('to_dense', DenseTransformer())\n",
    "        ])),\n",
    "        ('hour_of_day', HourOfDayTransformer()),\n",
    "        ('month', Pipeline([\n",
    "            ('extract', ColumnExtractor(['datetime'])),\n",
    "            ('to_month', DateTransformer()),\n",
    "            ('one_hot', OneHotEncoder()),\n",
    "            ('to_dense', DenseTransformer())\n",
    "        ])),\n",
    "        ('growth', Pipeline([\n",
    "            ('datetime', ColumnExtractor(['datetime'])),\n",
    "            ('to_numeric', MatrixConversion(int)),\n",
    "            ('regression', ModelTransformer(LinearRegression()))\n",
    "        ]))\n",
    "    ])),\n",
    "    ('estimators', FeatureUnion([\n",
    "        ('knn', ModelTransformer(KNeighborsRegressor(n_neighbors=5))),\n",
    "        ('gbr', ModelTransformer(GradientBoostingRegressor())),\n",
    "        ('dtr', ModelTransformer(DecisionTreeRegressor())),\n",
    "        ('etr', ModelTransformer(ExtraTreesRegressor())),\n",
    "        ('rfr', ModelTransformer(RandomForestRegressor())),\n",
    "        ('par', ModelTransformer(PassiveAggressiveRegressor())),\n",
    "        ('en', ModelTransformer(ElasticNet())),\n",
    "        ('cluster', ModelTransformer(KMeans(n_clusters=2)))\n",
    "    ])),\n",
    "    ('estimator', KNeighborsRegressor())\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9a4e7d17-95bf-4864-a1e9-9740265f63fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "#test ride ... works! Sweet. \n",
    "add_dummies = DFAddDummiesNaN(cols = unk_cat_feats)\n",
    "\n",
    "add_dummies.transform(data)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a36601cd-067d-44da-91a7-dc592f4848df",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "t = [('num', SimpleImputer(strategy='median'), [0, 1]), ('cat', SimpleImputer(strategy='most_frequent'), [2, 3])]\n",
    "transformer = ColumnTransformer(transformers=t, remainder='passthrough')\n",
    "#transformer = ColumnTransformer(transformers=[('cat', OneHotEncoder(), [2, 3])], remainder='passthrough')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "011ea416-41cf-4163-bd1f-d395b5818410",
   "metadata": {},
   "outputs": [],
   "source": [
    "#let me now follow https://machinelearningmastery.com/modeling-pipeline-optimization-with-scikit-learn/\n",
    "#which does cv = 3 instead of cv = stratififedkfold( ... )\n",
    "\n",
    "\n",
    "y = data_mean.loc[:, target]\n",
    "X= data_mean.loc[:, model_feats]\n",
    "y = y.astype(int)\n",
    "# Split the data into test and train\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X,  \n",
    "    y, \n",
    "    test_size=1/3,\n",
    "    random_state=0)\n",
    " \n",
    "print(X_train.shape)\n",
    "print(X_test.shape)\n",
    "\n",
    "#np.array(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "54d3b09a-ed5b-4321-9826-9de479282289",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5582b265-decb-4559-9526-0c4cbe68637f",
   "metadata": {},
   "outputs": [],
   "source": [
    "knn = KNeighborsClassifier()\n",
    "\n",
    "knn.fit(X_train, np.ravel(y_train))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7ea3f87c-5c84-4380-b452-0a8a19906f66",
   "metadata": {},
   "outputs": [],
   "source": [
    "knn.score(X_test, np.ravel(y_test))  #77% ? shitty! ha ha was 80% in my original version"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e9d58e4c-d0a1-44ab-bb90-596a75e627d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "##Sat Sept 18, 1:30 pm ... make pipe line? or do tuning and feature selection? "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f57a9cea-98b6-48b5-a0a9-f7e971cf4b36",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.linear_model import LogisticRegression \n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "rfc = RandomForestClassifier(max_depth=4, random_state=0)\n",
    "gnb = GaussianNB()\n",
    "lgc = LogisticRegression()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "96a3945a-6fff-4544-95c8-544240126f37",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "rfc.fit(X_train, np.ravel(y_train))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5bac71bd-fe79-4347-8ce2-a90f42907937",
   "metadata": {},
   "outputs": [],
   "source": [
    "rfc.score(X_test, np.ravel(y_test))  ##aha! I used rfc before and it IS improved to 82% ... cool. That makes me feel better ... (all that effort should not make it worse)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8f1c8a26-ab46-4119-95d1-8b607d9a7ee3",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "lgc.fit(X_train, np.ravel(y_train))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "28d30bba-3df5-4ef6-835d-c8bbea2f38a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "lgc.score(X_test, np.ravel(y_test))  ##aha! I used rfc before and it IS improved to 82% ... cool. That makes me feel better ... (all that effort should not make it worse)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7335f49f-5b0b-485f-8aaf-50d2d8fa91a6",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
