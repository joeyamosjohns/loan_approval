{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b1560cfb-6589-4bfe-9b8f-8103a1fa7925",
   "metadata": {},
   "outputs": [],
   "source": [
    "##Round 1 cleaning complete. v1 decisions: keep all nan as new feature/value; no skew or outlier action has been taken\n",
    "##Round 1 exploration complete. (a) Distribution of each feature explored (b) feature-target and feature-feature relationships explored \n",
    "\n",
    "##Round 1 modelling Plan: make one model for each notebook, try to do systematic tuning ... take best few models (?) or is it too soon?\n",
    "##then go back and do feature engineering and feature selection based on these base-line models\n",
    "##Build pipe-line: dummies, scaling, tuning ...\n",
    "\n",
    "##next steps:\n",
    "##\n",
    "##Flask API Deployment\n",
    "##build small webpage interaction\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1359efb1-103d-4360-a79d-3fceb18e9e4e",
   "metadata": {},
   "source": [
    "### Tuning of KNN \n",
    "\n",
    "Main parameters: k = num_neighbors, and metric"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8a8ea7be-9e6e-4828-a3b3-c987068772ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "## following https://machinelearningmastery.com/hyperparameters-for-classification-machine-learning-algorithms/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6d3930bc-de41-4da9-b866-9925090d5c99",
   "metadata": {},
   "outputs": [],
   "source": [
    "##brief review of KNN vs K-means ... \n",
    "##K-means (unspervised) has no labels, just feature space ... you create k-labels (choice of k is tricky and important)\n",
    "##then run thru k-means clustering algorithm to assign k-labels (clusters) to the data \n",
    "##b\n",
    "##KNN (supervised) is simpler because data already comes with p-labels (the role of k is different here) \n",
    "##in the training data on the test data you simply look at the k-nearet neighors (of training set) and take majority vote \n",
    "##on what label to assign to new point. The choice of k is important and the metric can change the outcomes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "d36736eb-dfca-4e86-9fb5-5ac94b4893b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "from matplotlib import pyplot as plt\n",
    "\n",
    "from sklearn.preprocessing import FunctionTransformer\n",
    "from sklearn.compose import ColumnTransformer\n",
    "\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.preprocessing import StandardScaler, RobustScaler, QuantileTransformer\n",
    "from sklearn.feature_selection import SelectKBest, f_regression\n",
    "from sklearn.decomposition import PCA \n",
    "from sklearn.linear_model import Ridge\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, roc_auc_score, confusion_matrix\n",
    "from sklearn.metrics import mean_absolute_error, mean_squared_error, r2_score, f1_score\n",
    "import seaborn as sns\n",
    "\n",
    "import pickle\n",
    "\n",
    "#sns.set()\n",
    "sns.set(rc={'figure.figsize':(8,5)})\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "98fec286-67d0-43e2-9388-f752a6f0cffc",
   "metadata": {},
   "outputs": [],
   "source": [
    "## cleaning tool\n",
    "\n",
    "\n",
    "def perc_null(X):\n",
    "    \n",
    "    total = X.isnull().sum().sort_values(ascending=False)\n",
    "    data_types = X.dtypes\n",
    "    percent = (X.isnull().sum()/X.isnull().count()).sort_values(ascending=False)\n",
    "\n",
    "    missing_data = pd.concat([total, data_types, percent], axis=1, keys=['Total','Type' ,'Percent'])\n",
    "    return missing_data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "84ed78ec-e3a5-42c6-978d-f79aa86412ad",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(614, 13)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Loan_ID</th>\n",
       "      <th>Gender</th>\n",
       "      <th>Married</th>\n",
       "      <th>Dependents</th>\n",
       "      <th>Education</th>\n",
       "      <th>Self_Employed</th>\n",
       "      <th>ApplicantIncome</th>\n",
       "      <th>CoapplicantIncome</th>\n",
       "      <th>LoanAmount</th>\n",
       "      <th>Loan_Amount_Term</th>\n",
       "      <th>Credit_History</th>\n",
       "      <th>Property_Area</th>\n",
       "      <th>Loan_Status</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>LP001002</td>\n",
       "      <td>Male</td>\n",
       "      <td>No</td>\n",
       "      <td>0</td>\n",
       "      <td>Graduate</td>\n",
       "      <td>No</td>\n",
       "      <td>5849</td>\n",
       "      <td>0.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>360.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>Urban</td>\n",
       "      <td>Y</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>LP001003</td>\n",
       "      <td>Male</td>\n",
       "      <td>Yes</td>\n",
       "      <td>1</td>\n",
       "      <td>Graduate</td>\n",
       "      <td>No</td>\n",
       "      <td>4583</td>\n",
       "      <td>1508.0</td>\n",
       "      <td>128.0</td>\n",
       "      <td>360.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>Rural</td>\n",
       "      <td>N</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>LP001005</td>\n",
       "      <td>Male</td>\n",
       "      <td>Yes</td>\n",
       "      <td>0</td>\n",
       "      <td>Graduate</td>\n",
       "      <td>Yes</td>\n",
       "      <td>3000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>66.0</td>\n",
       "      <td>360.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>Urban</td>\n",
       "      <td>Y</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    Loan_ID Gender Married Dependents Education Self_Employed  \\\n",
       "0  LP001002   Male      No          0  Graduate            No   \n",
       "1  LP001003   Male     Yes          1  Graduate            No   \n",
       "2  LP001005   Male     Yes          0  Graduate           Yes   \n",
       "\n",
       "   ApplicantIncome  CoapplicantIncome  LoanAmount  Loan_Amount_Term  \\\n",
       "0             5849                0.0         NaN             360.0   \n",
       "1             4583             1508.0       128.0             360.0   \n",
       "2             3000                0.0        66.0             360.0   \n",
       "\n",
       "   Credit_History Property_Area Loan_Status  \n",
       "0             1.0         Urban           Y  \n",
       "1             1.0         Rural           N  \n",
       "2             1.0         Urban           Y  "
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "path = \"/Users/joejohns/Downloads/data_loans.csv\"\n",
    "\n",
    "data = pd.read_csv(path) \n",
    "print(data.shape)\n",
    "data.head(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "63641633-d69d-4139-b4c9-6c46878e7554",
   "metadata": {},
   "source": [
    "## data processing from round 1\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "753f03c8-e92b-4f38-8474-f52e6e5d7f7c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Total</th>\n",
       "      <th>Type</th>\n",
       "      <th>Percent</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>Credit_History</th>\n",
       "      <td>50</td>\n",
       "      <td>float64</td>\n",
       "      <td>0.081433</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Self_Employed</th>\n",
       "      <td>32</td>\n",
       "      <td>object</td>\n",
       "      <td>0.052117</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>LoanAmount</th>\n",
       "      <td>22</td>\n",
       "      <td>float64</td>\n",
       "      <td>0.035831</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Dependents</th>\n",
       "      <td>15</td>\n",
       "      <td>object</td>\n",
       "      <td>0.024430</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Loan_Amount_Term</th>\n",
       "      <td>14</td>\n",
       "      <td>float64</td>\n",
       "      <td>0.022801</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Gender</th>\n",
       "      <td>13</td>\n",
       "      <td>object</td>\n",
       "      <td>0.021173</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Married</th>\n",
       "      <td>3</td>\n",
       "      <td>object</td>\n",
       "      <td>0.004886</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Loan_ID</th>\n",
       "      <td>0</td>\n",
       "      <td>object</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Education</th>\n",
       "      <td>0</td>\n",
       "      <td>object</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>ApplicantIncome</th>\n",
       "      <td>0</td>\n",
       "      <td>int64</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>CoapplicantIncome</th>\n",
       "      <td>0</td>\n",
       "      <td>float64</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Property_Area</th>\n",
       "      <td>0</td>\n",
       "      <td>object</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Loan_Status</th>\n",
       "      <td>0</td>\n",
       "      <td>object</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                   Total     Type   Percent\n",
       "Credit_History        50  float64  0.081433\n",
       "Self_Employed         32   object  0.052117\n",
       "LoanAmount            22  float64  0.035831\n",
       "Dependents            15   object  0.024430\n",
       "Loan_Amount_Term      14  float64  0.022801\n",
       "Gender                13   object  0.021173\n",
       "Married                3   object  0.004886\n",
       "Loan_ID                0   object  0.000000\n",
       "Education              0   object  0.000000\n",
       "ApplicantIncome        0    int64  0.000000\n",
       "CoapplicantIncome      0  float64  0.000000\n",
       "Property_Area          0   object  0.000000\n",
       "Loan_Status            0   object  0.000000"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "perc_null(data)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "f80c7b0a-593c-4b8e-a460-f09ab9d24362",
   "metadata": {},
   "outputs": [],
   "source": [
    "data.rename(columns = {'ApplicantIncome':'Applicant_Income', 'CoapplicantIncome': 'Coapplicant_Income', 'LoanAmount':'Loan_Amount'}, inplace = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "a701a37a-5017-43e5-b6a1-3437af02d9fd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['Loan_ID', 'Gender', 'Married', 'Dependents', 'Education',\n",
       "       'Self_Employed', 'Applicant_Income', 'Coapplicant_Income',\n",
       "       'Loan_Amount', 'Loan_Amount_Term', 'Credit_History', 'Property_Area',\n",
       "       'Loan_Status'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "559e805f-16fc-4026-b313-491a1701a735",
   "metadata": {},
   "outputs": [],
   "source": [
    "target = ['Loan_Status']\n",
    "ID_feats = ['Loan_ID',] \n",
    "\n",
    "#feature types:\n",
    "\n",
    "num_feats = ['Applicant_Income', 'Coapplicant_Income', 'Loan_Amount', 'Loan_Amount_Term',]\n",
    "\n",
    "ord_cat_feats = [ 'Dependents', 'Education', ]\n",
    "nom_cat_feats = ['Gender', 'Married', 'Self_Employed', 'Credit_History', 'Property_Area',]\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "#create new category values\n",
    "make_new_catval_unk = [  'Credit_History', 'Self_Employed',  'Dependents', 'Gender', 'Married', ] \n",
    "\n",
    "#create new category\n",
    "\n",
    "make_new_cat_unk = [ 'Loan_Amount', 'Loan_Amount_Term',]\n",
    "\n",
    "#if any value is unknown for either feature we make it 'yes')\n",
    "#cat = 'missing_data'; in general we can have several lists of related variables and make a new cat for each list; \n",
    "#name of cat should be specified\n",
    "\n",
    "#imputing\n",
    "\n",
    "impute_mean_feats = ['Loan_Amount']\n",
    "impute_freq_feats = [ 'Loan_Amount_Term']\n",
    "\n",
    "\n",
    "#scaling\n",
    "scaling_feats = ord_cat_feats+num_feats\n",
    "\n",
    "#dummies\n",
    "dummies_feats = nom_cat_feats #default setting of this variable ... \n",
    "#ordinal_encode\n",
    "ord_feats = ord_cat_feats\n",
    "\n",
    "#rest of categoricals get new catgory value 'unk' ... it is not very predictive in most cases ..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "54531acd-6157-40b6-8b63-583939eafe46",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "for item in ['0', '1', '2', '3+']:\n",
    "    if item != '3+':\n",
    "        i = int(item)\n",
    "    else:\n",
    "        i = 3\n",
    "    data.loc[data['Dependents'] == item,'Dependents']  = i\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "cfe69aee-b624-49d9-9c48-1feb284bcda2",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "data.loc[data[\"Loan_Status\"] == 'Y', \"Loan_Status\"] = 1\n",
    "data.loc[data[\"Loan_Status\"] == 'N', \"Loan_Status\"] = 0\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "1c26cfa6-f4c4-4a90-b5f7-8740c543ad98",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "for item in ['0', '1', '2', '3+']:\n",
    "    if item != '3+':\n",
    "        i = int(item)\n",
    "    else:\n",
    "        i = 3\n",
    "    data.loc[data['Dependents'] == item,'Dependents']  = i\n",
    "    \n",
    "#impute_most_freq = SimpleImputer(missing_values=np.nan,strategy=\"most_frequent\")\n",
    "#data[\"Dependents\"] = impute_most_freq.fit_transform(data[\"Dependents\"])\n",
    "\n",
    "#gah f-off with your 2D array non-sense ... I'll write my own function\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "886bbd77-fcc9-448a-bb1a-d2d6cc5ba096",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "data.loc[data['Credit_History'] ==0.0, 'Credit_History'] = 'No'\n",
    "data.loc[data['Credit_History'] == 1.0, 'Credit_History'] = 'Yes'  #I use capitalized \"Yes\", \"No\" to be conistent with their other entries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "1063d82e-3420-441b-a3da-711426fac445",
   "metadata": {},
   "outputs": [],
   "source": [
    "data.loc[data['Education']=='Not Graduate', \"Education\"] = 0\n",
    "data.loc[data['Education']=='Graduate', \"Education\"] = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "0f8de773-47a7-411c-b436-475b4ed767b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "#we don't want to do this for cat_ord_feats, because those will be numeric; we do impute_most_freq instead\n",
    "for cat in nom_cat_feats:\n",
    "    if data[cat].isnull().sum()>0: \n",
    "        data.loc[data[cat].isnull(), cat] = 'unk'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "8aa4b520-2f7e-43ff-8195-13213a4af2fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "## These are the two numerical features with NaN .. here we set up two new categories to capture these NaN since\n",
    "## they seem decently predictive. ... Now what to do with the NaN values? I guess just do mean and most-common\n",
    "\n",
    "\n",
    "#data['Loan_Amount_Term_unk'] = 'No' #set default to 'No' then set a few to be 'Yes'\n",
    "\n",
    "#create one new categorical for both (if either or both is unknown)\n",
    "data['missing_data'] = 'No'  \n",
    "data.loc[(data['Loan_Amount_Term'].isnull()) , 'missing_data'] = 'Yes'\n",
    "data.loc[(data['Loan_Amount'].isnull()) , 'missing_data'] = 'Yes'\n",
    "data.loc[(data['Gender'].isnull()) , 'missing_data'] = 'Yes'\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "61fdf942-067d-457c-b293-0e956227a6c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "#update nom_cat_feats (no null values, ok)\n",
    "\n",
    "nom_cat_feats+= ['missing_data']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6a682719-f833-44ff-a210-6376ec93a483",
   "metadata": {},
   "source": [
    "#### Done with cleaning categorical features ... \n",
    "note: we have made preliminary decision to make all NaN in categorical columns into new cat_val ... \\\n",
    "we'll see how this works out ... in some cases it seems to be a good predictor.\n",
    "\n",
    "In phase 2 investigation, below, we will investigate the naive predictive strength of all the \\\n",
    "categorical features (including NaN as 'unk')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "ca16fef6-74e2-49a6-a6a5-5d4819fdccc4",
   "metadata": {},
   "outputs": [],
   "source": [
    "##baseline models\n",
    "from sklearn.model_selection import train_test_split # For train/test splits\n",
    "from sklearn.neighbors import KNeighborsClassifier # The k-nearest neighbor classifier\n",
    "from sklearn.feature_selection import VarianceThreshold # Feature selector\n",
    "from sklearn.pipeline import Pipeline # For setting up pipeline\n",
    "# Various pre-processing steps\n",
    "from sklearn.preprocessing import Normalizer, StandardScaler, MinMaxScaler, PowerTransformer, MaxAbsScaler, LabelEncoder\n",
    "\n",
    "from sklearn.model_selection import RepeatedStratifiedKFold\n",
    "from sklearn.model_selection import RandomizedSearchCV\n",
    "\n",
    "from sklearn.model_selection import GridSearchCV # For optimization\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c753f775-82d7-4c3d-9bdc-015614af2543",
   "metadata": {},
   "source": [
    "pipeline should do this:\n",
    "\n",
    "num_prep = Pipeline([\n",
    "      #  ('keep_num', keep_cat),\n",
    "        ('most_freq_imp', most_freq_imp),\n",
    "        ('mean_imp', mean_imp), \n",
    "        ])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "45b4c135-a376-4600-9f4a-ebd3b7a1f042",
   "metadata": {},
   "outputs": [],
   "source": [
    "##if only one action taken in the pipe-line then don't need it.\n",
    "\n",
    "#all categoricals (which excludes ordinal cats) will be dummies\n",
    "\n",
    "\n",
    "#for the two numerical features, one should be most freq, one should be mean \n",
    "#most_freq_imp = Pipeline([\n",
    "      #  ('keep_num', keep_cat),\n",
    "#        ('most_freq_imp', most_freq_imp),\n",
    "        \n",
    "       # ('mean_imp', mean_imp), \n",
    "#        ])\n",
    "\n",
    "#mean_imp = Pipeline([\n",
    "      #  ('keep_num', keep_cat),\n",
    "        #('most_freq_imp', most_freq_imp),\n",
    "#        ('mean_imp', mean_imp), \n",
    "#        ])\n",
    "\n",
    "\n",
    "\n",
    "#cat_dummies = Pipeline([        \n",
    "       # ('keep_cat', keep_cat),\n",
    "        #('cat_imp', cat_imp),\n",
    "#        ('dummies', dummies),\n",
    "#        ])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "c5d100fc-69a4-4887-b3c4-70d99b9227d6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['Loan_ID', 'Gender', 'Married', 'Dependents', 'Education',\n",
       "       'Self_Employed', 'Applicant_Income', 'Coapplicant_Income',\n",
       "       'Loan_Amount', 'Loan_Amount_Term', 'Credit_History', 'Property_Area',\n",
       "       'Loan_Status', 'Loan_info_unk'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.columns"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "af9cb76c-7bb8-4710-a729-a5c5770a757e",
   "metadata": {},
   "source": [
    "target = ['Loan_Status']\n",
    "ID_feats = ['Loan_ID']\n",
    "#all features excepy ID and Loan_Status (to be used for modelling)\n",
    "\n",
    "model_feats= ['Gender', 'Married', 'Dependents', 'Education',\n",
    "       'Self_Employed', 'Applicant_Income', 'Coapplicant_Income',\n",
    "       'Loan_Amount', 'Loan_Amount_Term', 'Credit_History', 'Property_Area',\n",
    "        'Loan_Amount_Term_unk', 'Loan_Amount_unk']\n",
    "\n",
    "##categorical features\n",
    "\n",
    "'''cat_feats = [ 'Gender', 'Married', 'Dependents', 'Education',\n",
    "       'Self_Employed',  'Credit_History', 'Property_Area',\n",
    "        'Loan_Amount_Term_unk', 'Loan_Amount_unk'  ]\n",
    "\n",
    "cat_nom_feats = ['Gender', 'Married','Self_Employed',  'Credit_History', 'Property_Area',\n",
    "        'Loan_Amount_Term_unk', 'Loan_Amount_unk' ]\n",
    "cat_ord_feats = ['Dependents', 'Education',]\n",
    "\n",
    "##numerical features \n",
    "\n",
    "num_feats = ['Applicant_Income', 'Coapplicant_Income',\n",
    "       'Loan_Amount', 'Loan_Amount_Term', ]\n",
    "\n",
    "\n",
    "num_mean_imp_feats = [  'Applicant_Income', 'Coapplicant_Income', 'Loan_Amount']\n",
    "\n",
    "num_most_freq_imp_feats = ['Loan_Amount_Term',]'''\n",
    "\n",
    "\n",
    "#3 options for cat_feats with NaN:\n",
    "#-drop all NaN rows; or a column\n",
    "#-make NaN into new cat_val = 'unk'\n",
    "#-impute NaN by most common \n",
    "#-impute by most common of k-nearest nbrs\n",
    "\n",
    "#after NaN dealt with, then dealing with ordinal and nominal cats ... \n",
    "#which categories to do dummies and which to do scaling\n",
    "\n",
    "#cat_ord --> convert to 0,1,2, ... or possibly numbers which have correct SCALE relative to what they are encoding.\n",
    "cat_scale_feats = cat_ord_feats\n",
    "cat_dummies_feats = cat_nom_feats \n",
    "\n",
    "##numerical features\n",
    "\n",
    "num_feats = ['Applicant_Income', 'Coapplicant_Income',\n",
    "       'Loan_Amount', 'Loan_Amount_Term', ]\n",
    "\n",
    "# options for num_feats with NaN:\n",
    "#-impute by mean\n",
    "#-impute NaN by most common \n",
    "#-impute by mean of k-nearest nbrs ... \n",
    "#-drop all NaN rows or a column\n",
    "#-make into cat_var by binning; then make NaN into new cat_val = 'unk'\n",
    "#-add new category with cat_val = 'unk_yes', 'unk_no'; then still have to impute the cts NaN values \n",
    "\n",
    "\n",
    "num_mean_imp_feats = [  'Applicant_Income', 'Coapplicant_Income', 'Loan_Amount']\n",
    "num_most_freq_imp_feats = ['Loan_Amount_Term',]\n",
    "\n",
    "#scaling\n",
    "\n",
    "num_scale_feats = num_feats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bb04eca7-f754-4230-b719-7b9d51f4aab3",
   "metadata": {},
   "outputs": [],
   "source": [
    "##this class under construction ... continue when we get the rest working first\n",
    "\n",
    "class Nan_unk_Transformer():\n",
    "\n",
    "    # here you define the operation it should perform\n",
    "    def transform(self, X, y=None, **fit_params):\n",
    "        X2 = X.copy()\n",
    "        X2.loc[X.isnull(), :] = 'unk'\n",
    "        return X2 \n",
    "\n",
    "    # just return self\n",
    "    def fit(self, X, y=None, **fit_params):\n",
    "        return self\n",
    "\n",
    "cat_nom_feats = ['Gender', 'Married','Self_Employed',  'Credit_History', 'Property_Area',\n",
    "        'Loan_Amount_Term_unk', 'Loan_Amount_unk' ]\n",
    "\n",
    "#we don't want to do this for cat_ord_feats, because those will be numeric; we do impute_most_freq instead\n",
    "for cat in cat_nom_feats:\n",
    "    if data[cat].isnull().sum()>0: \n",
    "        data.loc[data[cat].isnull(), cat] = 'unk'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "430be1e1-3846-4ecd-9ebe-4f25e7a73216",
   "metadata": {},
   "source": [
    "#re-doing the cell below ...\n",
    "\n",
    "###I guess we have to do process1= pipe(...), process2= pipe(...), ...\n",
    "##then preprocess = columntransformer[(process1, cols1), (process2, cols2) ...]\n",
    "##processj can be: impute, data compress, feature select, make dummies,  ordnal encoder, scale ... \n",
    "##then pipe = pipeline[preprocess, model]\n",
    "\n",
    "##we will mix up categorical and numerical and simply focus on common processes, eg scaling for ordinal variables \n",
    "\n",
    "process: ordinal_encoding (done by hand for now)\n",
    "process: encoding categoricakl NaN ---> 'unk' (done by hand for now)\n",
    "process: create new cat for numerical NaN (done by hand for now)\n",
    "\n",
    "process: impute_most_freq, num_feat ={length of term}\n",
    "process: impute_mean, num_feats\n",
    "process: dummies, nom_cats\n",
    "process: scaling,  num_cats+ord_cats\n",
    "    \n",
    "    \n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "95cd7628-284a-4b6c-9309-8a185b76c1a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "freq_imp = SimpleImputer(missing_values=np.nan,strategy=\"most_frequent\")\n",
    "mean_imp = SimpleImputer(missing_values=np.nan, strategy=\"mean\")\n",
    "scaler = StandardScaler()\n",
    "dummies = OneHotEncoder(handle_unknown='ignore')\n",
    "\n",
    "#other options:\n",
    "#1. create new class that has transform and fit and uses make_dummies instead \n",
    "#2. put a FunctionTransformer() in the pipeline \n",
    "\n",
    "\n",
    "\n",
    "'''cat_impute_most_freq_feats = cat_ord_feats.copy()\n",
    "cat_scale_feats = cat_ord_feats.copy()\n",
    "cat_dummies_feats = cat_nom_feats.copy() \n",
    "num_scale_feats = num_feats.copy()\n",
    "mean_imp_feats = [  'Applicant_Income', 'Coapplicant_Income', 'Loan_Amount']\n",
    "impute_most_freq_feats = ['Loan_Amount_Term',]\n",
    "\n",
    "num_mean_imp_feats = [  'Applicant_Income', 'Coapplicant_Income', 'Loan_Amount']\n",
    "num_impute_most_freq_feats = ['Loan_Amount_Term',]\n",
    "'''\n",
    "\n",
    "\n",
    "#imputing\n",
    "\n",
    "impute_mean_feats = ['Loan_Amount','Gender']\n",
    "impute_freq_feats = [ 'Loan_Amount_Term']\n",
    "\n",
    "\n",
    "#scaling\n",
    "scaling_feats = ord_cat_feats+num_feats\n",
    "\n",
    "#dummies\n",
    "dummies_feats = nom_cat_feats #default setting of this variable ... \n",
    "#ordinal_encode\n",
    "ord_feats = ord_cat_feats  #done manually at this point\n",
    "\n",
    "preprocess = ColumnTransformer([\n",
    "    ('most_freq_imp', freq_imp, impute_freq_feats),\n",
    "    ('mean_imp', mean_imp, impute_mean_feats), #only one numerical feature has NaN right now\n",
    "    ('scaler', scaler, scaling_feats),\n",
    "    ('dummies', dummies, dummies_feats), \n",
    "])\n",
    "\n",
    "pipeline1 = Pipeline([\n",
    "    ('preprocess', preprocess),\n",
    "    ('knn', KNeighborsClassifier())\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "543e14f3-56ea-4878-bae7-67d216cc8428",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "##! Should set up a class which does NaN --> 'unk' imputer method\n",
    "\n",
    "\n",
    "\n",
    "##set up pipeline; here are the functions we'll use\n",
    "\n",
    "freq_imp = SimpleImputer(missing_values=np.nan,strategy=\"most_frequent\")\n",
    "mean_imp = SimpleImputer(missing_values=np.nan, strategy=\"mean\")\n",
    "scaler = StandardScaler()\n",
    "dummies = OneHotEncoder(handle_unknown='ignore')\n",
    "\n",
    "\n",
    "cat_impute_most_freq_feats = ord_cat_feats.copy()\n",
    "cat_scale_feats = ord_cat_feats.copy()\n",
    "cat_dummies_feats = nom_cat_feats.copy() \n",
    "\n",
    "\n",
    "num_scale_feats = num_feats.copy()\n",
    "\n",
    "num_mean_imp_feats = [  'Applicant_Income', 'Coapplicant_Income', 'Loan_Amount']\n",
    "num_impute_most_freq_feats = ['Loan_Amount_Term',]\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "#no need to impute for categoricals because already did NaN --> 'unk'\n",
    "#! should turn this into a class and use it's method\n",
    "\n",
    "\n",
    "categorical_preprocessing = ColumnTransformer([\n",
    "    ('cat_impute_most_freq', freq_imp, cat_impute_most_freq_feats),\n",
    "    ('cat_scale', scaler,  cat_scale_feats),\n",
    "    ('cat_dummies', dummies,  cat_dummies_feats),   \n",
    "])\n",
    "\n",
    "numerical_preprocessing = ColumnTransformer([\n",
    "    ('num_impute_most_freq', freq_imp, num_impute_most_freq_feats ),   #('name', method, [columns])\n",
    "    ('num_impute_mean', mean_imp,  num_mean_imp_feats),\n",
    "    ('num_scale', scaler, num_scale_feats)   \n",
    "])\n",
    " \n",
    "\n",
    "preprocess = Pipeline([\n",
    "    ('numerical_preprocessing', numerical_preprocessing),   #I switched the order of these two, so dummies is done at the end ... still messes up\n",
    "    ('categorical_preprocessing', categorical_preprocessing),\n",
    "    \n",
    "])\n",
    " \n",
    "\n",
    "#this was my old version ... did cat and num preprocess all mixed up\n",
    "'''# define which transformer applies to which columns\n",
    "preprocess = ColumnTransformer([\n",
    "    ('most_freq_imp', most_freq_imp, num_most_freq_imp),\n",
    "    ('mean_imp', mean_imp, num_feats), #only one numerical feature has NaN right now\n",
    "    ('scaler', scaler, num_feats+num_most_freq_imp),\n",
    "    ('dummies', dummies, cat_feats), \n",
    "])'''\n",
    "\n",
    "pipeline2 = Pipeline([\n",
    "    ('preprocess', preprocess),\n",
    "    ('knn', KNeighborsClassifier())\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "011ea416-41cf-4163-bd1f-d395b5818410",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(409, 12)\n",
      "(205, 12)\n"
     ]
    }
   ],
   "source": [
    "#let me now follow https://machinelearningmastery.com/modeling-pipeline-optimization-with-scikit-learn/\n",
    "#which does cv = 3 instead of cv = stratififedkfold( ... )\n",
    "\n",
    "model_feats = ord_cat_feats+nom_cat_feats+num_feats\n",
    "\n",
    "y = data.loc[:, target]\n",
    "X= data.loc[:, model_feats]\n",
    "\n",
    "# Split the data into test and train\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X,  \n",
    "    y, \n",
    "    test_size=1/3,\n",
    "    random_state=0)\n",
    " \n",
    "print(X_train.shape)\n",
    "print(X_test.shape)\n",
    "\n",
    "#np.array(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "ca0e2015-16db-477d-a9ba-613ad9bef4b5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Total</th>\n",
       "      <th>Type</th>\n",
       "      <th>Percent</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>Loan_Amount</th>\n",
       "      <td>22</td>\n",
       "      <td>float64</td>\n",
       "      <td>0.035831</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Dependents</th>\n",
       "      <td>15</td>\n",
       "      <td>object</td>\n",
       "      <td>0.024430</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Loan_Amount_Term</th>\n",
       "      <td>14</td>\n",
       "      <td>float64</td>\n",
       "      <td>0.022801</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Loan_ID</th>\n",
       "      <td>0</td>\n",
       "      <td>object</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Gender</th>\n",
       "      <td>0</td>\n",
       "      <td>object</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Married</th>\n",
       "      <td>0</td>\n",
       "      <td>object</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Education</th>\n",
       "      <td>0</td>\n",
       "      <td>object</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Self_Employed</th>\n",
       "      <td>0</td>\n",
       "      <td>object</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Applicant_Income</th>\n",
       "      <td>0</td>\n",
       "      <td>int64</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Coapplicant_Income</th>\n",
       "      <td>0</td>\n",
       "      <td>float64</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Credit_History</th>\n",
       "      <td>0</td>\n",
       "      <td>object</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Property_Area</th>\n",
       "      <td>0</td>\n",
       "      <td>object</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Loan_Status</th>\n",
       "      <td>0</td>\n",
       "      <td>object</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>missing_data</th>\n",
       "      <td>0</td>\n",
       "      <td>object</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                    Total     Type   Percent\n",
       "Loan_Amount            22  float64  0.035831\n",
       "Dependents             15   object  0.024430\n",
       "Loan_Amount_Term       14  float64  0.022801\n",
       "Loan_ID                 0   object  0.000000\n",
       "Gender                  0   object  0.000000\n",
       "Married                 0   object  0.000000\n",
       "Education               0   object  0.000000\n",
       "Self_Employed           0   object  0.000000\n",
       "Applicant_Income        0    int64  0.000000\n",
       "Coapplicant_Income      0  float64  0.000000\n",
       "Credit_History          0   object  0.000000\n",
       "Property_Area           0   object  0.000000\n",
       "Loan_Status             0   object  0.000000\n",
       "missing_data            0   object  0.000000"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "perc_null(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a0cd6eef-0fc9-4db1-b523-7b8d31d62a3c",
   "metadata": {},
   "outputs": [],
   "source": [
    "##see stack overflow ... guy who says \"Just to add ...\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bd904b7c-ec70-4a81-88d5-a26c9d2287b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "knn = pipeline2.fit(X_train, y_train)\n",
    "print('Training set score: ' + str(knn.score(X_train,y_train)))\n",
    "print('Test set score: ' + str(knn.score(X_test,y_test)))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "0afbcb17-b61b-4be8-ae04-dccd54382e4d",
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "A given column is not a column of the dataframe",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.8/site-packages/pandas/core/indexes/base.py\u001b[0m in \u001b[0;36mget_loc\u001b[0;34m(self, key, method, tolerance)\u001b[0m\n\u001b[1;32m   3360\u001b[0m             \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 3361\u001b[0;31m                 \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_engine\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_loc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcasted_key\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   3362\u001b[0m             \u001b[0;32mexcept\u001b[0m \u001b[0mKeyError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0merr\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.8/site-packages/pandas/_libs/index.pyx\u001b[0m in \u001b[0;36mpandas._libs.index.IndexEngine.get_loc\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.8/site-packages/pandas/_libs/index.pyx\u001b[0m in \u001b[0;36mpandas._libs.index.IndexEngine.get_loc\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;32mpandas/_libs/hashtable_class_helper.pxi\u001b[0m in \u001b[0;36mpandas._libs.hashtable.PyObjectHashTable.get_item\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;32mpandas/_libs/hashtable_class_helper.pxi\u001b[0m in \u001b[0;36mpandas._libs.hashtable.PyObjectHashTable.get_item\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;31mKeyError\u001b[0m: 'missing_data'",
      "\nThe above exception was the direct cause of the following exception:\n",
      "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.8/site-packages/sklearn/utils/__init__.py\u001b[0m in \u001b[0;36m_get_column_indices\u001b[0;34m(X, key)\u001b[0m\n\u001b[1;32m    395\u001b[0m             \u001b[0;32mfor\u001b[0m \u001b[0mcol\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mcolumns\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 396\u001b[0;31m                 \u001b[0mcol_idx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mall_columns\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_loc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcol\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    397\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcol_idx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnumbers\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mIntegral\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.8/site-packages/pandas/core/indexes/base.py\u001b[0m in \u001b[0;36mget_loc\u001b[0;34m(self, key, method, tolerance)\u001b[0m\n\u001b[1;32m   3362\u001b[0m             \u001b[0;32mexcept\u001b[0m \u001b[0mKeyError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0merr\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 3363\u001b[0;31m                 \u001b[0;32mraise\u001b[0m \u001b[0mKeyError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0merr\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   3364\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyError\u001b[0m: 'missing_data'",
      "\nThe above exception was the direct cause of the following exception:\n",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-52-be9e5af22216>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mknn\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpipeline1\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_train\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'Training set score: '\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mknn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mscore\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_train\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0my_train\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'Test set score: '\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mknn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mscore\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_test\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0my_test\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.8/site-packages/sklearn/pipeline.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, X, y, **fit_params)\u001b[0m\n\u001b[1;32m    339\u001b[0m         \"\"\"\n\u001b[1;32m    340\u001b[0m         \u001b[0mfit_params_steps\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_check_fit_params\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m**\u001b[0m\u001b[0mfit_params\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 341\u001b[0;31m         \u001b[0mXt\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_fit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mfit_params_steps\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    342\u001b[0m         with _print_elapsed_time('Pipeline',\n\u001b[1;32m    343\u001b[0m                                  self._log_message(len(self.steps) - 1)):\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.8/site-packages/sklearn/pipeline.py\u001b[0m in \u001b[0;36m_fit\u001b[0;34m(self, X, y, **fit_params_steps)\u001b[0m\n\u001b[1;32m    301\u001b[0m                 \u001b[0mcloned_transformer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mclone\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtransformer\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    302\u001b[0m             \u001b[0;31m# Fit or load from cache the current transformer\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 303\u001b[0;31m             X, fitted_transformer = fit_transform_one_cached(\n\u001b[0m\u001b[1;32m    304\u001b[0m                 \u001b[0mcloned_transformer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    305\u001b[0m                 \u001b[0mmessage_clsname\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'Pipeline'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.8/site-packages/joblib/memory.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    350\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    351\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__call__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 352\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    353\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    354\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mcall_and_shelve\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.8/site-packages/sklearn/pipeline.py\u001b[0m in \u001b[0;36m_fit_transform_one\u001b[0;34m(transformer, X, y, weight, message_clsname, message, **fit_params)\u001b[0m\n\u001b[1;32m    752\u001b[0m     \u001b[0;32mwith\u001b[0m \u001b[0m_print_elapsed_time\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmessage_clsname\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmessage\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    753\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mhasattr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtransformer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'fit_transform'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 754\u001b[0;31m             \u001b[0mres\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtransformer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit_transform\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mfit_params\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    755\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    756\u001b[0m             \u001b[0mres\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtransformer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mfit_params\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtransform\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.8/site-packages/sklearn/compose/_column_transformer.py\u001b[0m in \u001b[0;36mfit_transform\u001b[0;34m(self, X, y)\u001b[0m\n\u001b[1;32m    503\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_validate_transformers\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    504\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_validate_column_callables\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 505\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_validate_remainder\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    506\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    507\u001b[0m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_fit_transform\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_fit_transform_one\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.8/site-packages/sklearn/compose/_column_transformer.py\u001b[0m in \u001b[0;36m_validate_remainder\u001b[0;34m(self, X)\u001b[0m\n\u001b[1;32m    330\u001b[0m         \u001b[0mcols\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    331\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mcolumns\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_columns\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 332\u001b[0;31m             \u001b[0mcols\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mextend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m_get_column_indices\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcolumns\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    333\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    334\u001b[0m         \u001b[0mremaining_idx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msorted\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_n_features\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0mset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcols\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.8/site-packages/sklearn/utils/__init__.py\u001b[0m in \u001b[0;36m_get_column_indices\u001b[0;34m(X, key)\u001b[0m\n\u001b[1;32m    401\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    402\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mKeyError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 403\u001b[0;31m             raise ValueError(\n\u001b[0m\u001b[1;32m    404\u001b[0m                 \u001b[0;34m\"A given column is not a column of the dataframe\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    405\u001b[0m             ) from e\n",
      "\u001b[0;31mValueError\u001b[0m: A given column is not a column of the dataframe"
     ]
    }
   ],
   "source": [
    "\n",
    "knn = pipeline1.fit(X_train, y_train)\n",
    "print('Training set score: ' + str(knn.score(X_train,y_train)))\n",
    "print('Test set score: ' + str(knn.score(X_test,y_test)))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ee377f87-3d5a-40e1-ac3d-90b973754d40",
   "metadata": {},
   "outputs": [],
   "source": [
    "#This code snipet fails with the error ValueError: Supported target types are: ('binary', 'multiclass'). Got 'unknown' instead.\n",
    "#Here I am trying to mix these two: \n",
    "#just grid search (code below) https://machinelearningmastery.com/hyperparameters-for-classification-machine-learning-algorithms/\n",
    "#pipeline + gridsearch https://machinelearningmastery.com/modeling-pipeline-optimization-with-scikit-learn/\n",
    "\n",
    "#specifically I am using cv = nice stratified thing from the first one\n",
    "#in the second one he does test/train split then does grid search with cv =3 \n",
    "\n",
    "\n",
    "#according to https://chrisalbon.com/code/machine_learning/model_evaluation/cross_validation_pipeline/\n",
    "\n",
    "\n",
    "#Important note from the scikit docs: For integer/None inputs, if y is binary or multiclass, StratifiedKFold used. \n",
    "#    If the estimator is a classifier or if y is neither binary nor multiclass, KFold is used.\n",
    "\n",
    "\n",
    "# that is, if the estimator is a classifier then kfold and not stratified kfold is used ... ?? wtf ... they \n",
    "#should still use stratified kfold ... in fact in principle they should stratify by *all* categorified features ... always\n",
    "\n",
    "\n",
    "code = '''y = data.loc[:, target]\n",
    "X= data.loc[:, model_feats]\n",
    "\n",
    "\n",
    "#model = pipeline\n",
    "n_neighbors = range(1, 21, 2)\n",
    "weights = ['uniform', 'distance']\n",
    "metrics = ['euclidean', 'manhattan', 'minkowski']\n",
    "# define grid search\n",
    "grid = dict(n_neighbors=n_neighbors,weights=weights,metric=metrics)\n",
    "cv = RepeatedStratifiedKFold(n_splits=10, n_repeats=3, random_state=1)\n",
    "\n",
    "#grid = GridSearchCV(pipe, parameters, cv=2).fit(X_train, y_train)\n",
    "#y_train_categorical = keras.utils.to_categorical(y_train, num_classes)\n",
    "\n",
    "\n",
    "grid_search = GridSearchCV(estimator=pipeline, param_grid=grid, n_jobs=-1, cv=cv, scoring='accuracy',error_score=0)\n",
    "\n",
    "grid_result = grid_search.fit(np.array(X), np.ravel(y))\n",
    "# summarize results\n",
    "print(\"Best: %f using %s\" % (grid_result.best_score_, grid_result.best_params_))\n",
    "means = grid_result.cv_results_['mean_test_score']\n",
    "stds = grid_result.cv_results_['std_test_score']\n",
    "params = grid_result.cv_results_['params']\n",
    "for mean, stdev, param in zip(means, stds, params):\n",
    "    print(\"%f (%f) with: %r\" % (mean, stdev, param))'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a9a9460c-aa84-40c3-a6c7-496f2f887097",
   "metadata": {},
   "outputs": [],
   "source": [
    "'''import keras\n",
    "\n",
    "num_classes = len(set(y))\n",
    "y_train_categorical = keras.utils.to_categorical(y_train, num_classes)'''\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "51ab000a-7d4a-4d99-87cc-cf9532695ab8",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "edb2ae9a-7817-458c-b8a3-b8d7b98d5e05",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
